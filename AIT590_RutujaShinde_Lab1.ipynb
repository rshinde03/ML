{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AIT 590 NLP LAB 1**\n",
    "## By RUTUJA SHINDE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapping the text from webpage Using Beautiful Soup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs                                                           # BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scrape_webpage(url):\n",
    "       \n",
    "    scraped_textdata = urllib.request.urlopen(url)\n",
    "    textdata = scraped_textdata.read()\n",
    "    parsed_textdata = bs.BeautifulSoup(textdata,'lxml')\n",
    "    paragraphs = parsed_textdata.find_all('p')\n",
    "    formated_text = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        formated_text += para.text\n",
    "    \n",
    "    return formated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = _scrape_webpage('https://en.wikipedia.org/wiki/Natural_language_processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8602"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(mytext)\n",
    "len(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TOKENIZATION, PUNCTUATION, DIGIT AND STOP WORD REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8499\n"
     ]
    }
   ],
   "source": [
    "a = re.sub( r'\\d+', '', mytext)\n",
    "#print(a)\n",
    "#print(len(mytext))                               # digit removal\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after word tokenizing:  1441\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data']\n",
      "8499\n",
      "8602\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(a)\n",
    "print(\"Number of words after word tokenizing: \", len(tokens))                 #tokenizing the words\n",
    "print(tokens[:50])\n",
    "print(len(a))\n",
    "print(len(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Punctuation Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after word tokenizing with removing punctuation:  1270\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'information', 'engineering', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech']\n",
      "8499\n",
      "8602\n",
      "1270\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "regexp_tokens = tokenizer.tokenize(a.lower())                                   #punctuation removal\n",
    "print(\"Number of words after word tokenizing with removing punctuation: \", len(regexp_tokens))\n",
    "print(regexp_tokens[0:50])\n",
    "print(len(a))\n",
    "print(len(mytext))\n",
    "print(len(regexp_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stop Words Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rutuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rutuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words without stop words:  790\n",
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'information', 'engineering', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'history', 'natural', 'language', 'processing', 'nlp', 'generally', 'started', 'although']\n",
      "8499\n",
      "8602\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_tokens = [token for token in regexp_tokens if token not in stopwords.words('english')]\n",
    "print(\"# of words without stop words: \", len(stopwords_tokens))\n",
    "print(stopwords_tokens[0:50])                                                               #stopword removal\n",
    "print(len(a))\n",
    "print(len(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rutuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural => Natural\n",
      "language => language\n",
      "processing => processing\n",
      "( => (\n",
      "NLP => NLP\n",
      ") => )\n",
      "is => be\n",
      "a => a\n",
      "subfield => subfield\n",
      "of => of\n",
      "linguistics => linguistics\n",
      ", => ,\n",
      "computer => computer\n",
      "science => science\n",
      ", => ,\n",
      "information => information\n",
      "engineering => engineering\n",
      ", => ,\n",
      "and => and\n",
      "artificial => artificial\n",
      "intelligence => intelligence\n",
      "concerned => concern\n",
      "with => with\n",
      "the => the\n",
      "interactions => interaction\n",
      "between => between\n",
      "computers => computer\n",
      "and => and\n",
      "human => human\n",
      "( => (\n",
      "natural => natural\n",
      ") => )\n",
      "languages => language\n",
      ", => ,\n",
      "in => in\n",
      "particular => particular\n",
      "how => how\n",
      "to => to\n",
      "program => program\n",
      "computers => computer\n",
      "to => to\n",
      "process => process\n",
      "and => and\n",
      "analyze => analyze\n",
      "large => large\n",
      "amounts => amount\n",
      "of => of\n",
      "natural => natural\n",
      "language => language\n",
      "data => data\n",
      ". => .\n",
      "Challenges => Challenges\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "frequently => frequently\n",
      "involve => involve\n",
      "speech => speech\n",
      "recognition => recognition\n",
      ", => ,\n",
      "natural => natural\n",
      "language => language\n",
      "understanding => understanding\n",
      ", => ,\n",
      "and => and\n",
      "natural => natural\n",
      "language => language\n",
      "generation => generation\n",
      ". => .\n",
      "The => The\n",
      "history => history\n",
      "of => of\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "( => (\n",
      "NLP => NLP\n",
      ") => )\n",
      "generally => generally\n",
      "started => start\n",
      "in => in\n",
      "the => the\n",
      "s => s\n",
      ", => ,\n",
      "although => although\n",
      "work => work\n",
      "can => can\n",
      "be => be\n",
      "found => find\n",
      "from => from\n",
      "earlier => early\n",
      "periods => period\n",
      ". => .\n",
      "In => In\n",
      ", => ,\n",
      "Alan => Alan\n",
      "Turing => Turing\n",
      "published => publish\n",
      "an => an\n",
      "article => article\n",
      "titled => title\n",
      "`` => ``\n",
      "Computing => Computing\n",
      "Machinery => Machinery\n",
      "and => and\n",
      "Intelligence => Intelligence\n",
      "'' => ''\n",
      "which => which\n",
      "proposed => propose\n",
      "what => what\n",
      "is => be\n",
      "now => now\n",
      "called => call\n",
      "the => the\n",
      "Turing => Turing\n",
      "test => test\n",
      "as => a\n",
      "a => a\n",
      "criterion => criterion\n",
      "of => of\n",
      "intelligence => intelligence\n",
      "[ => [\n",
      "clarification => clarification\n",
      "needed => need\n",
      "] => ]\n",
      ". => .\n",
      "The => The\n",
      "Georgetown => Georgetown\n",
      "experiment => experiment\n",
      "in => in\n",
      "involved => involve\n",
      "fully => fully\n",
      "automatic => automatic\n",
      "translation => translation\n",
      "of => of\n",
      "more => more\n",
      "than => than\n",
      "sixty => sixty\n",
      "Russian => Russian\n",
      "sentences => sentence\n",
      "into => into\n",
      "English => English\n",
      ". => .\n",
      "The => The\n",
      "authors => author\n",
      "claimed => claim\n",
      "that => that\n",
      "within => within\n",
      "three => three\n",
      "or => or\n",
      "five => five\n",
      "years => year\n",
      ", => ,\n",
      "machine => machine\n",
      "translation => translation\n",
      "would => would\n",
      "be => be\n",
      "a => a\n",
      "solved => solved\n",
      "problem => problem\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "However => However\n",
      ", => ,\n",
      "real => real\n",
      "progress => progress\n",
      "was => be\n",
      "much => much\n",
      "slower => slow\n",
      ", => ,\n",
      "and => and\n",
      "after => after\n",
      "the => the\n",
      "ALPAC => ALPAC\n",
      "report => report\n",
      "in => in\n",
      ", => ,\n",
      "which => which\n",
      "found => find\n",
      "that => that\n",
      "ten-year-long => ten-year-long\n",
      "research => research\n",
      "had => have\n",
      "failed => fail\n",
      "to => to\n",
      "fulfill => fulfill\n",
      "the => the\n",
      "expectations => expectation\n",
      ", => ,\n",
      "funding => fund\n",
      "for => for\n",
      "machine => machine\n",
      "translation => translation\n",
      "was => be\n",
      "dramatically => dramatically\n",
      "reduced => reduce\n",
      ". => .\n",
      "Little => Little\n",
      "further => further\n",
      "research => research\n",
      "in => in\n",
      "machine => machine\n",
      "translation => translation\n",
      "was => be\n",
      "conducted => conduct\n",
      "until => until\n",
      "the => the\n",
      "late => late\n",
      "s => s\n",
      "when => when\n",
      "the => the\n",
      "first => first\n",
      "statistical => statistical\n",
      "machine => machine\n",
      "translation => translation\n",
      "systems => system\n",
      "were => be\n",
      "developed => develop\n",
      ". => .\n",
      "Some => Some\n",
      "notably => notably\n",
      "successful => successful\n",
      "natural => natural\n",
      "language => language\n",
      "processing => process\n",
      "systems => system\n",
      "developed => develop\n",
      "in => in\n",
      "the => the\n",
      "s => s\n",
      "were => be\n",
      "SHRDLU => SHRDLU\n",
      ", => ,\n",
      "a => a\n",
      "natural => natural\n",
      "language => language\n",
      "system => system\n",
      "working => work\n",
      "in => in\n",
      "restricted => restrict\n",
      "`` => ``\n",
      "blocks => block\n",
      "worlds => world\n",
      "'' => ''\n",
      "with => with\n",
      "restricted => restricted\n",
      "vocabularies => vocabulary\n",
      ", => ,\n",
      "and => and\n",
      "ELIZA => ELIZA\n",
      ", => ,\n",
      "a => a\n",
      "simulation => simulation\n",
      "of => of\n",
      "a => a\n",
      "Rogerian => Rogerian\n",
      "psychotherapist => psychotherapist\n",
      ", => ,\n",
      "written => write\n",
      "by => by\n",
      "Joseph => Joseph\n",
      "Weizenbaum => Weizenbaum\n",
      "between => between\n",
      "and => and\n",
      ". => .\n",
      "Using => Using\n",
      "almost => almost\n",
      "no => no\n",
      "information => information\n",
      "about => about\n",
      "human => human\n",
      "thought => thought\n",
      "or => or\n",
      "emotion => emotion\n",
      ", => ,\n",
      "ELIZA => ELIZA\n",
      "sometimes => sometimes\n",
      "provided => provide\n",
      "a => a\n",
      "startlingly => startlingly\n",
      "human-like => human-like\n",
      "interaction => interaction\n",
      ". => .\n",
      "When => When\n",
      "the => the\n",
      "`` => ``\n",
      "patient => patient\n",
      "'' => ''\n",
      "exceeded => exceed\n",
      "the => the\n",
      "very => very\n",
      "small => small\n",
      "knowledge => knowledge\n",
      "base => base\n",
      ", => ,\n",
      "ELIZA => ELIZA\n",
      "might => might\n",
      "provide => provide\n",
      "a => a\n",
      "generic => generic\n",
      "response => response\n",
      ", => ,\n",
      "for => for\n",
      "example => example\n",
      ", => ,\n",
      "responding => respond\n",
      "to => to\n",
      "`` => ``\n",
      "My => My\n",
      "head => head\n",
      "hurts => hurt\n",
      "'' => ''\n",
      "with => with\n",
      "`` => ``\n",
      "Why => Why\n",
      "do => do\n",
      "you => you\n",
      "say => say\n",
      "your => your\n",
      "head => head\n",
      "hurts => hurt\n",
      "? => ?\n",
      "'' => ''\n",
      ". => .\n",
      "During => During\n",
      "the => the\n",
      "s => s\n",
      ", => ,\n",
      "many => many\n",
      "programmers => programmer\n",
      "began => begin\n",
      "to => to\n",
      "write => write\n",
      "`` => ``\n",
      "conceptual => conceptual\n",
      "ontologies => ontology\n",
      "'' => ''\n",
      ", => ,\n",
      "which => which\n",
      "structured => structure\n",
      "real-world => real-world\n",
      "information => information\n",
      "into => into\n",
      "computer-understandable => computer-understandable\n",
      "data => data\n",
      ". => .\n",
      "Examples => Examples\n",
      "are => be\n",
      "MARGIE => MARGIE\n",
      "( => (\n",
      "Schank => Schank\n",
      ", => ,\n",
      ") => )\n",
      ", => ,\n",
      "SAM => SAM\n",
      "( => (\n",
      "Cullingford => Cullingford\n",
      ", => ,\n",
      ") => )\n",
      ", => ,\n",
      "PAM => PAM\n",
      "( => (\n",
      "Wilensky => Wilensky\n",
      ", => ,\n",
      ") => )\n",
      ", => ,\n",
      "TaleSpin => TaleSpin\n",
      "( => (\n",
      "Meehan => Meehan\n",
      ", => ,\n",
      ") => )\n",
      ", => ,\n",
      "QUALM => QUALM\n",
      "( => (\n",
      "Lehnert => Lehnert\n",
      ", => ,\n",
      ") => )\n",
      ", => ,\n",
      "Politics => Politics\n",
      "( => (\n",
      "Carbonell => Carbonell\n",
      ", => ,\n",
      ") => )\n",
      ", => ,\n",
      "and => and\n",
      "Plot => Plot\n",
      "Units => Units\n",
      "( => (\n",
      "Lehnert => Lehnert\n",
      ") => )\n",
      ". => .\n",
      "During => During\n",
      "this => this\n",
      "time => time\n",
      ", => ,\n",
      "many => many\n",
      "chatterbots => chatterbots\n",
      "were => be\n",
      "written => write\n",
      "including => include\n",
      "PARRY => PARRY\n",
      ", => ,\n",
      "Racter => Racter\n",
      ", => ,\n",
      "and => and\n",
      "Jabberwacky => Jabberwacky\n",
      ". => .\n",
      "Up => Up\n",
      "to => to\n",
      "the => the\n",
      "s => s\n",
      ", => ,\n",
      "most => most\n",
      "natural => natural\n",
      "language => language\n",
      "processing => process\n",
      "systems => system\n",
      "were => be\n",
      "based => base\n",
      "on => on\n",
      "complex => complex\n",
      "sets => set\n",
      "of => of\n",
      "hand-written => hand-written\n",
      "rules => rule\n",
      ". => .\n",
      "Starting => Starting\n",
      "in => in\n",
      "the => the\n",
      "late => late\n",
      "s => s\n",
      ", => ,\n",
      "however => however\n",
      ", => ,\n",
      "there => there\n",
      "was => be\n",
      "a => a\n",
      "revolution => revolution\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "with => with\n",
      "the => the\n",
      "introduction => introduction\n",
      "of => of\n",
      "machine => machine\n",
      "learning => learning\n",
      "algorithms => algorithm\n",
      "for => for\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "This => This\n",
      "was => be\n",
      "due => due\n",
      "to => to\n",
      "both => both\n",
      "the => the\n",
      "steady => steady\n",
      "increase => increase\n",
      "in => in\n",
      "computational => computational\n",
      "power => power\n",
      "( => (\n",
      "see => see\n",
      "Moore => Moore\n",
      "'s => 's\n",
      "law => law\n",
      ") => )\n",
      "and => and\n",
      "the => the\n",
      "gradual => gradual\n",
      "lessening => lessening\n",
      "of => of\n",
      "the => the\n",
      "dominance => dominance\n",
      "of => of\n",
      "Chomskyan => Chomskyan\n",
      "theories => theory\n",
      "of => of\n",
      "linguistics => linguistics\n",
      "( => (\n",
      "e.g => e.g\n",
      ". => .\n",
      "transformational => transformational\n",
      "grammar => grammar\n",
      ") => )\n",
      ", => ,\n",
      "whose => whose\n",
      "theoretical => theoretical\n",
      "underpinnings => underpinnings\n",
      "discouraged => discourage\n",
      "the => the\n",
      "sort => sort\n",
      "of => of\n",
      "corpus => corpus\n",
      "linguistics => linguistics\n",
      "that => that\n",
      "underlies => underlie\n",
      "the => the\n",
      "machine-learning => machine-learning\n",
      "approach => approach\n",
      "to => to\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "Some => Some\n",
      "of => of\n",
      "the => the\n",
      "earliest-used => earliest-used\n",
      "machine => machine\n",
      "learning => learn\n",
      "algorithms => algorithm\n",
      ", => ,\n",
      "such => such\n",
      "as => a\n",
      "decision => decision\n",
      "trees => tree\n",
      ", => ,\n",
      "produced => produce\n",
      "systems => system\n",
      "of => of\n",
      "hard => hard\n",
      "if-then => if-then\n",
      "rules => rule\n",
      "similar => similar\n",
      "to => to\n",
      "existing => exist\n",
      "hand-written => hand-written\n",
      "rules => rule\n",
      ". => .\n",
      "However => However\n",
      ", => ,\n",
      "part-of-speech => part-of-speech\n",
      "tagging => tagging\n",
      "introduced => introduce\n",
      "the => the\n",
      "use => use\n",
      "of => of\n",
      "hidden => hidden\n",
      "Markov => Markov\n",
      "models => model\n",
      "to => to\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      ", => ,\n",
      "and => and\n",
      "increasingly => increasingly\n",
      ", => ,\n",
      "research => research\n",
      "has => have\n",
      "focused => focus\n",
      "on => on\n",
      "statistical => statistical\n",
      "models => model\n",
      ", => ,\n",
      "which => which\n",
      "make => make\n",
      "soft => soft\n",
      ", => ,\n",
      "probabilistic => probabilistic\n",
      "decisions => decision\n",
      "based => base\n",
      "on => on\n",
      "attaching => attach\n",
      "real-valued => real-valued\n",
      "weights => weight\n",
      "to => to\n",
      "the => the\n",
      "features => feature\n",
      "making => make\n",
      "up => up\n",
      "the => the\n",
      "input => input\n",
      "data => data\n",
      ". => .\n",
      "The => The\n",
      "cache => cache\n",
      "language => language\n",
      "models => model\n",
      "upon => upon\n",
      "which => which\n",
      "many => many\n",
      "speech => speech\n",
      "recognition => recognition\n",
      "systems => system\n",
      "now => now\n",
      "rely => rely\n",
      "are => be\n",
      "examples => example\n",
      "of => of\n",
      "such => such\n",
      "statistical => statistical\n",
      "models => model\n",
      ". => .\n",
      "Such => Such\n",
      "models => model\n",
      "are => be\n",
      "generally => generally\n",
      "more => more\n",
      "robust => robust\n",
      "when => when\n",
      "given => give\n",
      "unfamiliar => unfamiliar\n",
      "input => input\n",
      ", => ,\n",
      "especially => especially\n",
      "input => input\n",
      "that => that\n",
      "contains => contain\n",
      "errors => error\n",
      "( => (\n",
      "as => a\n",
      "is => be\n",
      "very => very\n",
      "common => common\n",
      "for => for\n",
      "real-world => real-world\n",
      "data => data\n",
      ") => )\n",
      ", => ,\n",
      "and => and\n",
      "produce => produce\n",
      "more => more\n",
      "reliable => reliable\n",
      "results => result\n",
      "when => when\n",
      "integrated => integrate\n",
      "into => into\n",
      "a => a\n",
      "larger => large\n",
      "system => system\n",
      "comprising => comprise\n",
      "multiple => multiple\n",
      "subtasks => subtasks\n",
      ". => .\n",
      "Many => Many\n",
      "of => of\n",
      "the => the\n",
      "notable => notable\n",
      "early => early\n",
      "successes => success\n",
      "occurred => occur\n",
      "in => in\n",
      "the => the\n",
      "field => field\n",
      "of => of\n",
      "machine => machine\n",
      "translation => translation\n",
      ", => ,\n",
      "due => due\n",
      "especially => especially\n",
      "to => to\n",
      "work => work\n",
      "at => at\n",
      "IBM => IBM\n",
      "Research => Research\n",
      ", => ,\n",
      "where => where\n",
      "successively => successively\n",
      "more => more\n",
      "complicated => complicated\n",
      "statistical => statistical\n",
      "models => model\n",
      "were => be\n",
      "developed => develop\n",
      ". => .\n",
      "These => These\n",
      "systems => system\n",
      "were => be\n",
      "able => able\n",
      "to => to\n",
      "take => take\n",
      "advantage => advantage\n",
      "of => of\n",
      "existing => exist\n",
      "multilingual => multilingual\n",
      "textual => textual\n",
      "corpora => corpus\n",
      "that => that\n",
      "had => have\n",
      "been => be\n",
      "produced => produce\n",
      "by => by\n",
      "the => the\n",
      "Parliament => Parliament\n",
      "of => of\n",
      "Canada => Canada\n",
      "and => and\n",
      "the => the\n",
      "European => European\n",
      "Union => Union\n",
      "as => a\n",
      "a => a\n",
      "result => result\n",
      "of => of\n",
      "laws => law\n",
      "calling => call\n",
      "for => for\n",
      "the => the\n",
      "translation => translation\n",
      "of => of\n",
      "all => all\n",
      "governmental => governmental\n",
      "proceedings => proceeding\n",
      "into => into\n",
      "all => all\n",
      "official => official\n",
      "languages => language\n",
      "of => of\n",
      "the => the\n",
      "corresponding => corresponding\n",
      "systems => system\n",
      "of => of\n",
      "government => government\n",
      ". => .\n",
      "However => However\n",
      ", => ,\n",
      "most => most\n",
      "other => other\n",
      "systems => system\n",
      "depended => depend\n",
      "on => on\n",
      "corpora => corpus\n",
      "specifically => specifically\n",
      "developed => develop\n",
      "for => for\n",
      "the => the\n",
      "tasks => task\n",
      "implemented => implement\n",
      "by => by\n",
      "these => these\n",
      "systems => system\n",
      ", => ,\n",
      "which => which\n",
      "was => be\n",
      "( => (\n",
      "and => and\n",
      "often => often\n",
      "continues => continue\n",
      "to => to\n",
      "be => be\n",
      ") => )\n",
      "a => a\n",
      "major => major\n",
      "limitation => limitation\n",
      "in => in\n",
      "the => the\n",
      "success => success\n",
      "of => of\n",
      "these => these\n",
      "systems => system\n",
      ". => .\n",
      "As => As\n",
      "a => a\n",
      "result => result\n",
      ", => ,\n",
      "a => a\n",
      "great => great\n",
      "deal => deal\n",
      "of => of\n",
      "research => research\n",
      "has => have\n",
      "gone => go\n",
      "into => into\n",
      "methods => method\n",
      "of => of\n",
      "more => more\n",
      "effectively => effectively\n",
      "learning => learn\n",
      "from => from\n",
      "limited => limited\n",
      "amounts => amount\n",
      "of => of\n",
      "data => data\n",
      ". => .\n",
      "Recent => Recent\n",
      "research => research\n",
      "has => have\n",
      "increasingly => increasingly\n",
      "focused => focus\n",
      "on => on\n",
      "unsupervised => unsupervised\n",
      "and => and\n",
      "semi-supervised => semi-supervised\n",
      "learning => learning\n",
      "algorithms => algorithm\n",
      ". => .\n",
      "Such => Such\n",
      "algorithms => algorithm\n",
      "can => can\n",
      "learn => learn\n",
      "from => from\n",
      "data => data\n",
      "that => that\n",
      "has => have\n",
      "not => not\n",
      "been => be\n",
      "hand-annotated => hand-annotated\n",
      "with => with\n",
      "the => the\n",
      "desired => desired\n",
      "answers => answer\n",
      "or => or\n",
      "using => use\n",
      "a => a\n",
      "combination => combination\n",
      "of => of\n",
      "annotated => annotated\n",
      "and => and\n",
      "non-annotated => non-annotated\n",
      "data => data\n",
      ". => .\n",
      "Generally => Generally\n",
      ", => ,\n",
      "this => this\n",
      "task => task\n",
      "is => be\n",
      "much => much\n",
      "more => more\n",
      "difficult => difficult\n",
      "than => than\n",
      "supervised => supervised\n",
      "learning => learning\n",
      ", => ,\n",
      "and => and\n",
      "typically => typically\n",
      "produces => produce\n",
      "less => less\n",
      "accurate => accurate\n",
      "results => result\n",
      "for => for\n",
      "a => a\n",
      "given => give\n",
      "amount => amount\n",
      "of => of\n",
      "input => input\n",
      "data => data\n",
      ". => .\n",
      "However => However\n",
      ", => ,\n",
      "there => there\n",
      "is => be\n",
      "an => an\n",
      "enormous => enormous\n",
      "amount => amount\n",
      "of => of\n",
      "non-annotated => non-annotated\n",
      "data => data\n",
      "available => available\n",
      "( => (\n",
      "including => include\n",
      ", => ,\n",
      "among => among\n",
      "other => other\n",
      "things => thing\n",
      ", => ,\n",
      "the => the\n",
      "entire => entire\n",
      "content => content\n",
      "of => of\n",
      "the => the\n",
      "World => World\n",
      "Wide => Wide\n",
      "Web => Web\n",
      ") => )\n",
      ", => ,\n",
      "which => which\n",
      "can => can\n",
      "often => often\n",
      "make => make\n",
      "up => up\n",
      "for => for\n",
      "the => the\n",
      "inferior => inferior\n",
      "results => result\n",
      "if => if\n",
      "the => the\n",
      "algorithm => algorithm\n",
      "used => use\n",
      "has => have\n",
      "a => a\n",
      "low => low\n",
      "enough => enough\n",
      "time => time\n",
      "complexity => complexity\n",
      "to => to\n",
      "be => be\n",
      "practical => practical\n",
      ". => .\n",
      "In => In\n",
      "the => the\n",
      "s => s\n",
      ", => ,\n",
      "representation => representation\n",
      "learning => learning\n",
      "and => and\n",
      "deep => deep\n",
      "neural => neural\n",
      "network-style => network-style\n",
      "machine => machine\n",
      "learning => learn\n",
      "methods => method\n",
      "became => become\n",
      "widespread => widespread\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      ", => ,\n",
      "due => due\n",
      "in => in\n",
      "part => part\n",
      "to => to\n",
      "a => a\n",
      "flurry => flurry\n",
      "of => of\n",
      "results => result\n",
      "showing => show\n",
      "that => that\n",
      "such => such\n",
      "techniques => technique\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "can => can\n",
      "achieve => achieve\n",
      "state-of-the-art => state-of-the-art\n",
      "results => result\n",
      "in => in\n",
      "many => many\n",
      "natural => natural\n",
      "language => language\n",
      "tasks => task\n",
      ", => ,\n",
      "for => for\n",
      "example => example\n",
      "in => in\n",
      "language => language\n",
      "modeling => modeling\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "parsing => parsing\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "and => and\n",
      "many => many\n",
      "others => others\n",
      ". => .\n",
      "Popular => Popular\n",
      "techniques => technique\n",
      "include => include\n",
      "the => the\n",
      "use => use\n",
      "of => of\n",
      "word => word\n",
      "embeddings => embeddings\n",
      "to => to\n",
      "capture => capture\n",
      "semantic => semantic\n",
      "properties => property\n",
      "of => of\n",
      "words => word\n",
      ", => ,\n",
      "and => and\n",
      "an => an\n",
      "increase => increase\n",
      "in => in\n",
      "end-to-end => end-to-end\n",
      "learning => learning\n",
      "of => of\n",
      "a => a\n",
      "higher-level => higher-level\n",
      "task => task\n",
      "( => (\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "question => question\n",
      "answering => answer\n",
      ") => )\n",
      "instead => instead\n",
      "of => of\n",
      "relying => rely\n",
      "on => on\n",
      "a => a\n",
      "pipeline => pipeline\n",
      "of => of\n",
      "separate => separate\n",
      "intermediate => intermediate\n",
      "tasks => task\n",
      "( => (\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "part-of-speech => part-of-speech\n",
      "tagging => tagging\n",
      "and => and\n",
      "dependency => dependency\n",
      "parsing => parsing\n",
      ") => )\n",
      ". => .\n",
      "In => In\n",
      "some => some\n",
      "areas => area\n",
      ", => ,\n",
      "this => this\n",
      "shift => shift\n",
      "has => have\n",
      "entailed => entail\n",
      "substantial => substantial\n",
      "changes => change\n",
      "in => in\n",
      "how => how\n",
      "NLP => NLP\n",
      "systems => system\n",
      "are => be\n",
      "designed => design\n",
      ", => ,\n",
      "such => such\n",
      "that => that\n",
      "deep => deep\n",
      "neural => neural\n",
      "network-based => network-based\n",
      "approaches => approach\n",
      "may => may\n",
      "be => be\n",
      "viewed => view\n",
      "as => a\n",
      "a => a\n",
      "new => new\n",
      "paradigm => paradigm\n",
      "distinct => distinct\n",
      "from => from\n",
      "statistical => statistical\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "For => For\n",
      "instance => instance\n",
      ", => ,\n",
      "the => the\n",
      "term => term\n",
      "neural => neural\n",
      "machine => machine\n",
      "translation => translation\n",
      "( => (\n",
      "NMT => NMT\n",
      ") => )\n",
      "emphasizes => emphasize\n",
      "the => the\n",
      "fact => fact\n",
      "that => that\n",
      "deep => deep\n",
      "learning-based => learning-based\n",
      "approaches => approach\n",
      "to => to\n",
      "machine => machine\n",
      "translation => translation\n",
      "directly => directly\n",
      "learn => learn\n",
      "sequence-to-sequence => sequence-to-sequence\n",
      "transformations => transformation\n",
      ", => ,\n",
      "obviating => obviate\n",
      "the => the\n",
      "need => need\n",
      "for => for\n",
      "intermediate => intermediate\n",
      "steps => step\n",
      "such => such\n",
      "as => a\n",
      "word => word\n",
      "alignment => alignment\n",
      "and => and\n",
      "language => language\n",
      "modeling => modeling\n",
      "that => that\n",
      "was => be\n",
      "used => use\n",
      "in => in\n",
      "statistical => statistical\n",
      "machine => machine\n",
      "translation => translation\n",
      "( => (\n",
      "SMT => SMT\n",
      ") => )\n",
      ". => .\n",
      "In => In\n",
      "the => the\n",
      "early => early\n",
      "days => day\n",
      ", => ,\n",
      "many => many\n",
      "language-processing => language-processing\n",
      "systems => system\n",
      "were => be\n",
      "designed => design\n",
      "by => by\n",
      "hand-coding => hand-coding\n",
      "a => a\n",
      "set => set\n",
      "of => of\n",
      "rules => rule\n",
      ": => :\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "such => such\n",
      "as => a\n",
      "by => by\n",
      "writing => write\n",
      "grammars => grammar\n",
      "or => or\n",
      "devising => devise\n",
      "heuristic => heuristic\n",
      "rules => rule\n",
      "for => for\n",
      "stemming => stem\n",
      ". => .\n",
      "Since => Since\n",
      "the => the\n",
      "so-called => so-called\n",
      "`` => ``\n",
      "statistical => statistical\n",
      "revolution => revolution\n",
      "'' => ''\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "in => in\n",
      "the => the\n",
      "late => late\n",
      "s => s\n",
      "and => and\n",
      "mid-s => mid-s\n",
      ", => ,\n",
      "much => much\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "research => research\n",
      "has => have\n",
      "relied => rely\n",
      "heavily => heavily\n",
      "on => on\n",
      "machine => machine\n",
      "learning => learning\n",
      ". => .\n",
      "The => The\n",
      "machine-learning => machine-learning\n",
      "paradigm => paradigm\n",
      "calls => call\n",
      "instead => instead\n",
      "for => for\n",
      "using => use\n",
      "statistical => statistical\n",
      "inference => inference\n",
      "to => to\n",
      "automatically => automatically\n",
      "learn => learn\n",
      "such => such\n",
      "rules => rule\n",
      "through => through\n",
      "the => the\n",
      "analysis => analysis\n",
      "of => of\n",
      "large => large\n",
      "corpora => corpus\n",
      "( => (\n",
      "the => the\n",
      "plural => plural\n",
      "form => form\n",
      "of => of\n",
      "corpus => corpus\n",
      ", => ,\n",
      "is => be\n",
      "a => a\n",
      "set => set\n",
      "of => of\n",
      "documents => document\n",
      ", => ,\n",
      "possibly => possibly\n",
      "with => with\n",
      "human => human\n",
      "or => or\n",
      "computer => computer\n",
      "annotations => annotation\n",
      ") => )\n",
      "of => of\n",
      "typical => typical\n",
      "real-world => real-world\n",
      "examples => example\n",
      ". => .\n",
      "Many => Many\n",
      "different => different\n",
      "classes => class\n",
      "of => of\n",
      "machine-learning => machine-learning\n",
      "algorithms => algorithm\n",
      "have => have\n",
      "been => be\n",
      "applied => apply\n",
      "to => to\n",
      "natural-language-processing => natural-language-processing\n",
      "tasks => task\n",
      ". => .\n",
      "These => These\n",
      "algorithms => algorithm\n",
      "take => take\n",
      "as => a\n",
      "input => input\n",
      "a => a\n",
      "large => large\n",
      "set => set\n",
      "of => of\n",
      "`` => ``\n",
      "features => feature\n",
      "'' => ''\n",
      "that => that\n",
      "are => be\n",
      "generated => generate\n",
      "from => from\n",
      "the => the\n",
      "input => input\n",
      "data => data\n",
      ". => .\n",
      "Some => Some\n",
      "of => of\n",
      "the => the\n",
      "earliest-used => earliest-used\n",
      "algorithms => algorithm\n",
      ", => ,\n",
      "such => such\n",
      "as => a\n",
      "decision => decision\n",
      "trees => tree\n",
      ", => ,\n",
      "produced => produce\n",
      "systems => system\n",
      "of => of\n",
      "hard => hard\n",
      "if-then => if-then\n",
      "rules => rule\n",
      "similar => similar\n",
      "to => to\n",
      "the => the\n",
      "systems => system\n",
      "of => of\n",
      "handwritten => handwritten\n",
      "rules => rule\n",
      "that => that\n",
      "were => be\n",
      "then => then\n",
      "common => common\n",
      ". => .\n",
      "Increasingly => Increasingly\n",
      ", => ,\n",
      "however => however\n",
      ", => ,\n",
      "research => research\n",
      "has => have\n",
      "focused => focus\n",
      "on => on\n",
      "statistical => statistical\n",
      "models => model\n",
      ", => ,\n",
      "which => which\n",
      "make => make\n",
      "soft => soft\n",
      ", => ,\n",
      "probabilistic => probabilistic\n",
      "decisions => decision\n",
      "based => base\n",
      "on => on\n",
      "attaching => attach\n",
      "real-valued => real-valued\n",
      "weights => weight\n",
      "to => to\n",
      "each => each\n",
      "input => input\n",
      "feature => feature\n",
      ". => .\n",
      "Such => Such\n",
      "models => model\n",
      "have => have\n",
      "the => the\n",
      "advantage => advantage\n",
      "that => that\n",
      "they => they\n",
      "can => can\n",
      "express => express\n",
      "the => the\n",
      "relative => relative\n",
      "certainty => certainty\n",
      "of => of\n",
      "many => many\n",
      "different => different\n",
      "possible => possible\n",
      "answers => answer\n",
      "rather => rather\n",
      "than => than\n",
      "only => only\n",
      "one => one\n",
      ", => ,\n",
      "producing => produce\n",
      "more => more\n",
      "reliable => reliable\n",
      "results => result\n",
      "when => when\n",
      "such => such\n",
      "a => a\n",
      "model => model\n",
      "is => be\n",
      "included => include\n",
      "as => a\n",
      "a => a\n",
      "component => component\n",
      "of => of\n",
      "a => a\n",
      "larger => large\n",
      "system => system\n",
      ". => .\n",
      "Systems => Systems\n",
      "based => base\n",
      "on => on\n",
      "machine-learning => machine-learning\n",
      "algorithms => algorithm\n",
      "have => have\n",
      "many => many\n",
      "advantages => advantage\n",
      "over => over\n",
      "hand-produced => hand-produced\n",
      "rules => rule\n",
      ": => :\n",
      "The => The\n",
      "following => following\n",
      "is => be\n",
      "a => a\n",
      "list => list\n",
      "of => of\n",
      "some => some\n",
      "of => of\n",
      "the => the\n",
      "most => most\n",
      "commonly => commonly\n",
      "researched => researched\n",
      "tasks => task\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "Some => Some\n",
      "of => of\n",
      "these => these\n",
      "tasks => task\n",
      "have => have\n",
      "direct => direct\n",
      "real-world => real-world\n",
      "applications => application\n",
      ", => ,\n",
      "while => while\n",
      "others => others\n",
      "more => more\n",
      "commonly => commonly\n",
      "serve => serve\n",
      "as => a\n",
      "subtasks => subtasks\n",
      "that => that\n",
      "are => be\n",
      "used => use\n",
      "to => to\n",
      "aid => aid\n",
      "in => in\n",
      "solving => solve\n",
      "larger => large\n",
      "tasks => task\n",
      ". => .\n",
      "Though => Though\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "tasks => task\n",
      "are => be\n",
      "closely => closely\n",
      "intertwined => intertwine\n",
      ", => ,\n",
      "they => they\n",
      "are => be\n",
      "frequently => frequently\n",
      "subdivided => subdivide\n",
      "into => into\n",
      "categories => category\n",
      "for => for\n",
      "convenience => convenience\n",
      ". => .\n",
      "A => A\n",
      "coarse => coarse\n",
      "division => division\n",
      "is => be\n",
      "given => give\n",
      "below => below\n",
      ". => .\n",
      "The => The\n",
      "first => first\n",
      "published => publish\n",
      "work => work\n",
      "by => by\n",
      "an => an\n",
      "artificial => artificial\n",
      "intelligence => intelligence\n",
      "was => be\n",
      "published => publish\n",
      "in => in\n",
      ", => ,\n",
      "the => the\n",
      "Road => Road\n",
      ", => ,\n",
      "marketed => market\n",
      "as => a\n",
      "a => a\n",
      "novel => novel\n",
      ", => ,\n",
      "contains => contain\n",
      "sixty => sixty\n",
      "million => million\n",
      "words => word\n",
      ". => .\n",
      "8499\n",
      "8602\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text= stopwords_tokens\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token, tag_map[tag[0]])                         #lemmatization\n",
    "    print(token, \"=>\", lemma)\n",
    "    \n",
    "print(len(a))\n",
    "print(len(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PLOT TOP 15 WORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 437 samples and 790 outcomes>\n",
      "[('language', 24), ('natural', 19), ('processing', 16), ('machine', 16), ('systems', 15), ('learning', 14), ('data', 10), ('translation', 10), ('many', 10), ('statistical', 9), ('rules', 9), ('research', 8), ('algorithms', 8), ('models', 8), ('tasks', 8)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ed6669b6a0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist = nltk.FreqDist(stopwords_tokens)\n",
    "print(freq_dist)\n",
    "k = 15\n",
    "print(freq_dist.most_common(k))                       #plot the top 15 words in the text \n",
    "freq_dist.plot(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FreqDist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: 24\n",
      "natural: 19\n",
      "processing: 16\n",
      "machine: 16\n",
      "systems: 15\n",
      "learning: 14\n",
      "data: 10\n",
      "translation: 10\n",
      "many: 10\n",
      "statistical: 9\n",
      "rules: 9\n",
      "research: 8\n",
      "algorithms: 8\n",
      "models: 8\n",
      "tasks: 8\n"
     ]
    }
   ],
   "source": [
    "for word, frequency in freq_dist.most_common(k):                  #Calculate frequency of each word using freq Dist\n",
    "    print(u'{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist.B()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WEIGHTED FREQUENCY USING FreqDist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_weighted_frequency(freq_dist) -> dict:\n",
    "    #get the word frequency from FreqDist\n",
    "    word_freqs={}\n",
    "    for word in freq_dist.keys():\n",
    "        word_freqs[word] = freq_dist.get(word)\n",
    "    #find the maximum of the frequency\n",
    "    max_freq = freq_dist.most_common(1) [0] [1]\n",
    "    #find the total number of words in freq_dist                            #Weighted freq \n",
    "    total_num_words=freq_dist.B();\n",
    "    \n",
    "    #calculate weighted frequency of occurance\n",
    "    for word in word_freqs.keys():\n",
    "        word_freqs[word] =(word_freqs[word]/total_num_words)\n",
    "        print('%s' %word, ':' , word_freqs[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural : 0.043478260869565216\n",
      "language : 0.05491990846681922\n",
      "processing : 0.036613272311212815\n",
      "nlp : 0.006864988558352402\n",
      "subfield : 0.002288329519450801\n",
      "linguistics : 0.006864988558352402\n",
      "computer : 0.006864988558352402\n",
      "science : 0.002288329519450801\n",
      "information : 0.006864988558352402\n",
      "engineering : 0.002288329519450801\n",
      "artificial : 0.004576659038901602\n",
      "intelligence : 0.009153318077803204\n",
      "concerned : 0.002288329519450801\n",
      "interactions : 0.002288329519450801\n",
      "computers : 0.004576659038901602\n",
      "human : 0.009153318077803204\n",
      "languages : 0.004576659038901602\n",
      "particular : 0.002288329519450801\n",
      "program : 0.002288329519450801\n",
      "process : 0.002288329519450801\n",
      "analyze : 0.002288329519450801\n",
      "large : 0.006864988558352402\n",
      "amounts : 0.004576659038901602\n",
      "data : 0.02288329519450801\n",
      "challenges : 0.002288329519450801\n",
      "frequently : 0.004576659038901602\n",
      "involve : 0.002288329519450801\n",
      "speech : 0.009153318077803204\n",
      "recognition : 0.004576659038901602\n",
      "understanding : 0.002288329519450801\n",
      "generation : 0.002288329519450801\n",
      "history : 0.002288329519450801\n",
      "generally : 0.006864988558352402\n",
      "started : 0.002288329519450801\n",
      "although : 0.002288329519450801\n",
      "work : 0.006864988558352402\n",
      "found : 0.004576659038901602\n",
      "earlier : 0.002288329519450801\n",
      "periods : 0.002288329519450801\n",
      "alan : 0.002288329519450801\n",
      "turing : 0.004576659038901602\n",
      "published : 0.006864988558352402\n",
      "article : 0.002288329519450801\n",
      "titled : 0.002288329519450801\n",
      "computing : 0.002288329519450801\n",
      "machinery : 0.002288329519450801\n",
      "proposed : 0.002288329519450801\n",
      "called : 0.004576659038901602\n",
      "test : 0.002288329519450801\n",
      "criterion : 0.002288329519450801\n",
      "clarification : 0.002288329519450801\n",
      "needed : 0.002288329519450801\n",
      "georgetown : 0.002288329519450801\n",
      "experiment : 0.002288329519450801\n",
      "involved : 0.002288329519450801\n",
      "fully : 0.002288329519450801\n",
      "automatic : 0.002288329519450801\n",
      "translation : 0.02288329519450801\n",
      "sixty : 0.004576659038901602\n",
      "russian : 0.002288329519450801\n",
      "sentences : 0.002288329519450801\n",
      "english : 0.002288329519450801\n",
      "authors : 0.002288329519450801\n",
      "claimed : 0.002288329519450801\n",
      "within : 0.002288329519450801\n",
      "three : 0.002288329519450801\n",
      "five : 0.002288329519450801\n",
      "years : 0.002288329519450801\n",
      "machine : 0.036613272311212815\n",
      "would : 0.002288329519450801\n",
      "solved : 0.002288329519450801\n",
      "problem : 0.002288329519450801\n",
      "however : 0.013729977116704805\n",
      "real : 0.016018306636155607\n",
      "progress : 0.002288329519450801\n",
      "much : 0.006864988558352402\n",
      "slower : 0.002288329519450801\n",
      "alpac : 0.002288329519450801\n",
      "report : 0.002288329519450801\n",
      "ten : 0.002288329519450801\n",
      "year : 0.002288329519450801\n",
      "long : 0.002288329519450801\n",
      "research : 0.018306636155606407\n",
      "failed : 0.002288329519450801\n",
      "fulfill : 0.002288329519450801\n",
      "expectations : 0.002288329519450801\n",
      "funding : 0.002288329519450801\n",
      "dramatically : 0.002288329519450801\n",
      "reduced : 0.002288329519450801\n",
      "little : 0.002288329519450801\n",
      "conducted : 0.002288329519450801\n",
      "late : 0.006864988558352402\n",
      "first : 0.004576659038901602\n",
      "statistical : 0.020594965675057208\n",
      "systems : 0.034324942791762014\n",
      "developed : 0.009153318077803204\n",
      "notably : 0.002288329519450801\n",
      "successful : 0.002288329519450801\n",
      "shrdlu : 0.002288329519450801\n",
      "system : 0.006864988558352402\n",
      "working : 0.002288329519450801\n",
      "restricted : 0.004576659038901602\n",
      "blocks : 0.002288329519450801\n",
      "worlds : 0.002288329519450801\n",
      "vocabularies : 0.002288329519450801\n",
      "eliza : 0.006864988558352402\n",
      "simulation : 0.002288329519450801\n",
      "rogerian : 0.002288329519450801\n",
      "psychotherapist : 0.002288329519450801\n",
      "written : 0.009153318077803204\n",
      "joseph : 0.002288329519450801\n",
      "weizenbaum : 0.002288329519450801\n",
      "using : 0.006864988558352402\n",
      "almost : 0.002288329519450801\n",
      "thought : 0.002288329519450801\n",
      "emotion : 0.002288329519450801\n",
      "sometimes : 0.002288329519450801\n",
      "provided : 0.002288329519450801\n",
      "startlingly : 0.002288329519450801\n",
      "like : 0.002288329519450801\n",
      "interaction : 0.002288329519450801\n",
      "patient : 0.002288329519450801\n",
      "exceeded : 0.002288329519450801\n",
      "small : 0.002288329519450801\n",
      "knowledge : 0.002288329519450801\n",
      "base : 0.002288329519450801\n",
      "might : 0.002288329519450801\n",
      "provide : 0.002288329519450801\n",
      "generic : 0.002288329519450801\n",
      "response : 0.002288329519450801\n",
      "example : 0.004576659038901602\n",
      "responding : 0.002288329519450801\n",
      "head : 0.004576659038901602\n",
      "hurts : 0.004576659038901602\n",
      "say : 0.002288329519450801\n",
      "many : 0.02288329519450801\n",
      "programmers : 0.002288329519450801\n",
      "began : 0.002288329519450801\n",
      "write : 0.002288329519450801\n",
      "conceptual : 0.002288329519450801\n",
      "ontologies : 0.002288329519450801\n",
      "structured : 0.002288329519450801\n",
      "world : 0.011441647597254004\n",
      "understandable : 0.002288329519450801\n",
      "examples : 0.006864988558352402\n",
      "margie : 0.002288329519450801\n",
      "schank : 0.002288329519450801\n",
      "sam : 0.002288329519450801\n",
      "cullingford : 0.002288329519450801\n",
      "pam : 0.002288329519450801\n",
      "wilensky : 0.002288329519450801\n",
      "talespin : 0.002288329519450801\n",
      "meehan : 0.002288329519450801\n",
      "qualm : 0.002288329519450801\n",
      "lehnert : 0.004576659038901602\n",
      "politics : 0.002288329519450801\n",
      "carbonell : 0.002288329519450801\n",
      "plot : 0.002288329519450801\n",
      "units : 0.002288329519450801\n",
      "time : 0.004576659038901602\n",
      "chatterbots : 0.002288329519450801\n",
      "including : 0.004576659038901602\n",
      "parry : 0.002288329519450801\n",
      "racter : 0.002288329519450801\n",
      "jabberwacky : 0.002288329519450801\n",
      "based : 0.013729977116704805\n",
      "complex : 0.002288329519450801\n",
      "sets : 0.002288329519450801\n",
      "hand : 0.011441647597254004\n",
      "rules : 0.020594965675057208\n",
      "starting : 0.002288329519450801\n",
      "revolution : 0.004576659038901602\n",
      "introduction : 0.002288329519450801\n",
      "learning : 0.032036613272311214\n",
      "algorithms : 0.018306636155606407\n",
      "due : 0.006864988558352402\n",
      "steady : 0.002288329519450801\n",
      "increase : 0.004576659038901602\n",
      "computational : 0.002288329519450801\n",
      "power : 0.002288329519450801\n",
      "see : 0.002288329519450801\n",
      "moore : 0.002288329519450801\n",
      "law : 0.002288329519450801\n",
      "gradual : 0.002288329519450801\n",
      "lessening : 0.002288329519450801\n",
      "dominance : 0.002288329519450801\n",
      "chomskyan : 0.002288329519450801\n",
      "theories : 0.002288329519450801\n",
      "e : 0.006864988558352402\n",
      "g : 0.006864988558352402\n",
      "transformational : 0.002288329519450801\n",
      "grammar : 0.002288329519450801\n",
      "whose : 0.002288329519450801\n",
      "theoretical : 0.002288329519450801\n",
      "underpinnings : 0.002288329519450801\n",
      "discouraged : 0.002288329519450801\n",
      "sort : 0.002288329519450801\n",
      "corpus : 0.004576659038901602\n",
      "underlies : 0.002288329519450801\n",
      "approach : 0.002288329519450801\n",
      "earliest : 0.004576659038901602\n",
      "used : 0.011441647597254004\n",
      "decision : 0.004576659038901602\n",
      "trees : 0.004576659038901602\n",
      "produced : 0.009153318077803204\n",
      "hard : 0.004576659038901602\n",
      "similar : 0.004576659038901602\n",
      "existing : 0.004576659038901602\n",
      "part : 0.006864988558352402\n",
      "tagging : 0.004576659038901602\n",
      "introduced : 0.002288329519450801\n",
      "use : 0.004576659038901602\n",
      "hidden : 0.002288329519450801\n",
      "markov : 0.002288329519450801\n",
      "models : 0.018306636155606407\n",
      "increasingly : 0.006864988558352402\n",
      "focused : 0.006864988558352402\n",
      "make : 0.006864988558352402\n",
      "soft : 0.004576659038901602\n",
      "probabilistic : 0.004576659038901602\n",
      "decisions : 0.004576659038901602\n",
      "attaching : 0.004576659038901602\n",
      "valued : 0.004576659038901602\n",
      "weights : 0.004576659038901602\n",
      "features : 0.004576659038901602\n",
      "making : 0.002288329519450801\n",
      "input : 0.016018306636155607\n",
      "cache : 0.002288329519450801\n",
      "upon : 0.002288329519450801\n",
      "rely : 0.002288329519450801\n",
      "robust : 0.002288329519450801\n",
      "given : 0.006864988558352402\n",
      "unfamiliar : 0.002288329519450801\n",
      "especially : 0.004576659038901602\n",
      "contains : 0.004576659038901602\n",
      "errors : 0.002288329519450801\n",
      "common : 0.004576659038901602\n",
      "produce : 0.002288329519450801\n",
      "reliable : 0.004576659038901602\n",
      "results : 0.013729977116704805\n",
      "integrated : 0.002288329519450801\n",
      "larger : 0.006864988558352402\n",
      "comprising : 0.002288329519450801\n",
      "multiple : 0.002288329519450801\n",
      "subtasks : 0.004576659038901602\n",
      "notable : 0.002288329519450801\n",
      "early : 0.004576659038901602\n",
      "successes : 0.002288329519450801\n",
      "occurred : 0.002288329519450801\n",
      "field : 0.002288329519450801\n",
      "ibm : 0.002288329519450801\n",
      "successively : 0.002288329519450801\n",
      "complicated : 0.002288329519450801\n",
      "able : 0.002288329519450801\n",
      "take : 0.004576659038901602\n",
      "advantage : 0.004576659038901602\n",
      "multilingual : 0.002288329519450801\n",
      "textual : 0.002288329519450801\n",
      "corpora : 0.006864988558352402\n",
      "parliament : 0.002288329519450801\n",
      "canada : 0.002288329519450801\n",
      "european : 0.002288329519450801\n",
      "union : 0.002288329519450801\n",
      "result : 0.004576659038901602\n",
      "laws : 0.002288329519450801\n",
      "calling : 0.002288329519450801\n",
      "governmental : 0.002288329519450801\n",
      "proceedings : 0.002288329519450801\n",
      "official : 0.002288329519450801\n",
      "corresponding : 0.002288329519450801\n",
      "government : 0.002288329519450801\n",
      "depended : 0.002288329519450801\n",
      "specifically : 0.002288329519450801\n",
      "tasks : 0.018306636155606407\n",
      "implemented : 0.002288329519450801\n",
      "often : 0.004576659038901602\n",
      "continues : 0.002288329519450801\n",
      "major : 0.002288329519450801\n",
      "limitation : 0.002288329519450801\n",
      "success : 0.002288329519450801\n",
      "great : 0.002288329519450801\n",
      "deal : 0.002288329519450801\n",
      "gone : 0.002288329519450801\n",
      "methods : 0.004576659038901602\n",
      "effectively : 0.002288329519450801\n",
      "limited : 0.002288329519450801\n",
      "recent : 0.002288329519450801\n",
      "unsupervised : 0.002288329519450801\n",
      "semi : 0.002288329519450801\n",
      "supervised : 0.004576659038901602\n",
      "learn : 0.006864988558352402\n",
      "annotated : 0.009153318077803204\n",
      "desired : 0.002288329519450801\n",
      "answers : 0.004576659038901602\n",
      "combination : 0.002288329519450801\n",
      "non : 0.004576659038901602\n",
      "task : 0.004576659038901602\n",
      "difficult : 0.002288329519450801\n",
      "typically : 0.002288329519450801\n",
      "produces : 0.002288329519450801\n",
      "less : 0.002288329519450801\n",
      "accurate : 0.002288329519450801\n",
      "amount : 0.004576659038901602\n",
      "enormous : 0.002288329519450801\n",
      "available : 0.002288329519450801\n",
      "among : 0.002288329519450801\n",
      "things : 0.002288329519450801\n",
      "entire : 0.002288329519450801\n",
      "content : 0.002288329519450801\n",
      "wide : 0.002288329519450801\n",
      "web : 0.002288329519450801\n",
      "inferior : 0.002288329519450801\n",
      "algorithm : 0.002288329519450801\n",
      "low : 0.002288329519450801\n",
      "enough : 0.002288329519450801\n",
      "complexity : 0.002288329519450801\n",
      "practical : 0.002288329519450801\n",
      "representation : 0.002288329519450801\n",
      "deep : 0.006864988558352402\n",
      "neural : 0.006864988558352402\n",
      "network : 0.004576659038901602\n",
      "style : 0.002288329519450801\n",
      "became : 0.002288329519450801\n",
      "widespread : 0.002288329519450801\n",
      "flurry : 0.002288329519450801\n",
      "showing : 0.002288329519450801\n",
      "techniques : 0.004576659038901602\n",
      "achieve : 0.002288329519450801\n",
      "state : 0.002288329519450801\n",
      "art : 0.002288329519450801\n",
      "modeling : 0.004576659038901602\n",
      "parsing : 0.004576659038901602\n",
      "others : 0.004576659038901602\n",
      "popular : 0.002288329519450801\n",
      "include : 0.002288329519450801\n",
      "word : 0.004576659038901602\n",
      "embeddings : 0.002288329519450801\n",
      "capture : 0.002288329519450801\n",
      "semantic : 0.002288329519450801\n",
      "properties : 0.002288329519450801\n",
      "words : 0.004576659038901602\n",
      "end : 0.004576659038901602\n",
      "higher : 0.002288329519450801\n",
      "level : 0.002288329519450801\n",
      "question : 0.002288329519450801\n",
      "answering : 0.002288329519450801\n",
      "instead : 0.004576659038901602\n",
      "relying : 0.002288329519450801\n",
      "pipeline : 0.002288329519450801\n",
      "separate : 0.002288329519450801\n",
      "intermediate : 0.004576659038901602\n",
      "dependency : 0.002288329519450801\n",
      "areas : 0.002288329519450801\n",
      "shift : 0.002288329519450801\n",
      "entailed : 0.002288329519450801\n",
      "substantial : 0.002288329519450801\n",
      "changes : 0.002288329519450801\n",
      "designed : 0.004576659038901602\n",
      "approaches : 0.004576659038901602\n",
      "may : 0.002288329519450801\n",
      "viewed : 0.002288329519450801\n",
      "new : 0.002288329519450801\n",
      "paradigm : 0.004576659038901602\n",
      "distinct : 0.002288329519450801\n",
      "instance : 0.002288329519450801\n",
      "term : 0.002288329519450801\n",
      "nmt : 0.002288329519450801\n",
      "emphasizes : 0.002288329519450801\n",
      "fact : 0.002288329519450801\n",
      "directly : 0.002288329519450801\n",
      "sequence : 0.004576659038901602\n",
      "transformations : 0.002288329519450801\n",
      "obviating : 0.002288329519450801\n",
      "need : 0.002288329519450801\n",
      "steps : 0.002288329519450801\n",
      "alignment : 0.002288329519450801\n",
      "smt : 0.002288329519450801\n",
      "days : 0.002288329519450801\n",
      "coding : 0.002288329519450801\n",
      "set : 0.006864988558352402\n",
      "writing : 0.002288329519450801\n",
      "grammars : 0.002288329519450801\n",
      "devising : 0.002288329519450801\n",
      "heuristic : 0.002288329519450801\n",
      "stemming : 0.002288329519450801\n",
      "since : 0.002288329519450801\n",
      "mid : 0.002288329519450801\n",
      "relied : 0.002288329519450801\n",
      "heavily : 0.002288329519450801\n",
      "calls : 0.002288329519450801\n",
      "inference : 0.002288329519450801\n",
      "automatically : 0.002288329519450801\n",
      "analysis : 0.002288329519450801\n",
      "plural : 0.002288329519450801\n",
      "form : 0.002288329519450801\n",
      "documents : 0.002288329519450801\n",
      "possibly : 0.002288329519450801\n",
      "annotations : 0.002288329519450801\n",
      "typical : 0.002288329519450801\n",
      "different : 0.004576659038901602\n",
      "classes : 0.002288329519450801\n",
      "applied : 0.002288329519450801\n",
      "generated : 0.002288329519450801\n",
      "handwritten : 0.002288329519450801\n",
      "feature : 0.002288329519450801\n",
      "express : 0.002288329519450801\n",
      "relative : 0.002288329519450801\n",
      "certainty : 0.002288329519450801\n",
      "possible : 0.002288329519450801\n",
      "rather : 0.002288329519450801\n",
      "one : 0.002288329519450801\n",
      "producing : 0.002288329519450801\n",
      "model : 0.002288329519450801\n",
      "included : 0.002288329519450801\n",
      "component : 0.002288329519450801\n",
      "advantages : 0.002288329519450801\n",
      "following : 0.002288329519450801\n",
      "list : 0.002288329519450801\n",
      "commonly : 0.004576659038901602\n",
      "researched : 0.002288329519450801\n",
      "direct : 0.002288329519450801\n",
      "applications : 0.002288329519450801\n",
      "serve : 0.002288329519450801\n",
      "aid : 0.002288329519450801\n",
      "solving : 0.002288329519450801\n",
      "though : 0.002288329519450801\n",
      "closely : 0.002288329519450801\n",
      "intertwined : 0.002288329519450801\n",
      "subdivided : 0.002288329519450801\n",
      "categories : 0.002288329519450801\n",
      "convenience : 0.002288329519450801\n",
      "coarse : 0.002288329519450801\n",
      "division : 0.002288329519450801\n",
      "road : 0.002288329519450801\n",
      "marketed : 0.002288329519450801\n",
      "novel : 0.002288329519450801\n",
      "million : 0.002288329519450801\n"
     ]
    }
   ],
   "source": [
    "_find_weighted_frequency(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str= mytext\n",
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to make a frequency table to store frequencies of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frequency_table(txt_str) -> dict:\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(txt_str)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freq_table = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to create a matrix to store frequencies of the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frequency_matrix(sentences):\n",
    "    freq_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        freq_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return freq_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to perform TF and form a matrix of it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to form a bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define  a fucntion to perform IDF and form a matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to perform TF-IDF and store in matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),f_table2.items()):  \n",
    "                                                                            # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to Score the Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(tf_idf_matrix) -> dict:\n",
    "    \n",
    "\n",
    "    sentence_value = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentence_value[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentence_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to compute the average scores of the sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_average_score(sentence_value) -> int:\n",
    "    \n",
    "    sum_values = 0\n",
    "    for entry in sentence_value:\n",
    "        sum_values += sentence_value[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sum_values / len(sentence_value))\n",
    "\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to form a summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_summary(sentences, sentence_value, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentence_value and sentence_value[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calling all the  functions to perform sentence scoring, TF-IDF and summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summarization(text):\n",
    "    \n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    total_documents = len(sentences)                                            #Sentence Tokenize\n",
    "    print(sentences)\n",
    "    \n",
    "    freq_matrix = make_frequency_matrix(sentences)                #Create the Frequency matrix of the words in each sentence.\n",
    "    print(freq_matrix)\n",
    "\n",
    "   \n",
    "    \n",
    "    tf_matrix = make_tf_matrix(freq_matrix)                              #Calculate the term frequency and generate a matrix\n",
    "    print(tf_matrix)\n",
    "\n",
    "    \n",
    "    count_doc_per_words = make_documents_per_words(freq_matrix)            #create a table to store the documents per words\n",
    "    print(count_doc_per_words)\n",
    "\n",
    "    \n",
    "    \n",
    "    idf_matrix = make_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    print(idf_matrix)                                                             #Calculate the IDF and generate a matrix\n",
    "\n",
    "    \n",
    "    tf_idf_matrix = make_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    print(tf_idf_matrix)                                                        #Calculate the TF-IDF and generate a matrix\n",
    "\n",
    "    \n",
    "    sentence_scores = score_sentences(tf_idf_matrix)\n",
    "    print(sentence_scores)                                                  #call the function and score the sentences\n",
    "\n",
    "    \n",
    "    threshold = cal_average_score(sentence_scores)\n",
    "    print(threshold)                                                           #Find the threshold\n",
    "\n",
    "    \n",
    "    summary = form_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "    return summary                                                           #call the function and generate the summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRINT FINAL RESULT WITH SENTENCE SCORE, TF-IDF MATRIX AND SUMMARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.', 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.', 'The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods.', 'In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence[clarification needed].', 'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.', 'The authors claimed that within three or five years, machine translation would be a solved problem.', '[2]  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.', 'Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.', 'Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.', 'Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.', 'When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".', 'During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.', 'Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).', 'During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.', 'Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.', 'Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.', \"This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g.\", 'transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.', '[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.', 'However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.', 'The cache language models upon which many speech recognition systems now rely are examples of such statistical models.', 'Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.', 'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.', 'These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.', 'However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.', 'As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.', 'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.', 'Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.', 'Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.', 'However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.', 'In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.', 'Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).', 'In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.', 'For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).', 'In the early days, many language-processing systems were designed by hand-coding a set of rules:[9][10] such as by writing grammars or devising heuristic rules for stemming.', 'Since the so-called \"statistical revolution\"[11][12] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.', 'The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.', 'Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.', 'These algorithms take as input a large set of \"features\" that are generated from the input data.', 'Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of handwritten rules that were then common.', 'Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.', 'Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.', 'Systems based on machine-learning algorithms have many advantages over hand-produced rules:\\nThe following is a list of some of the most commonly researched tasks in natural language processing.', 'Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.', 'Though natural language processing tasks are closely intertwined, they are frequently subdivided into categories for convenience.', 'A coarse division is given below.', 'The first published work by an artificial intelligence was published in 2018, 1 the Road, marketed as a novel, contains sixty million words.']\n",
      "{'Natural languag': {'natur': 3, 'languag': 3, 'process': 2, '(': 2, 'nlp': 1, ')': 2, 'subfield': 1, 'linguist': 1, ',': 4, 'comput': 3, 'scienc': 1, 'inform': 1, 'engin': 1, 'artifici': 1, 'intellig': 1, 'concern': 1, 'interact': 1, 'human': 1, 'particular': 1, 'program': 1, 'analyz': 1, 'larg': 1, 'amount': 1, 'data': 1, '.': 1}, 'Challenges in n': {'challeng': 1, 'natur': 3, 'languag': 3, 'process': 1, 'frequent': 1, 'involv': 1, 'speech': 1, 'recognit': 1, ',': 2, 'understand': 1, 'gener': 1, '.': 1}, 'The history of ': {'histori': 1, 'natur': 1, 'languag': 1, 'process': 1, '(': 1, 'nlp': 1, ')': 1, 'gener': 1, 'start': 1, '1950': 1, ',': 1, 'although': 1, 'work': 1, 'found': 1, 'earlier': 1, 'period': 1, '.': 1}, 'In 1950, Alan T': {'1950': 1, ',': 1, 'alan': 1, 'ture': 2, 'publish': 1, 'articl': 1, 'titl': 1, '``': 1, 'comput': 1, 'machineri': 1, 'intellig': 2, \"''\": 1, 'propos': 1, 'call': 1, 'test': 1, 'criterion': 1, '[': 1, 'clarif': 1, 'need': 1, ']': 1, '.': 1}, 'The Georgetown ': {'georgetown': 1, 'experi': 1, '1954': 1, 'involv': 1, 'fulli': 1, 'automat': 1, 'translat': 1, 'sixti': 1, 'russian': 1, 'sentenc': 1, 'english': 1, '.': 1}, 'The authors cla': {'author': 1, 'claim': 1, 'within': 1, 'three': 1, 'five': 1, 'year': 1, ',': 1, 'machin': 1, 'translat': 1, 'would': 1, 'solv': 1, 'problem': 1, '.': 1}, '[2]  However, r': {'[': 1, '2': 1, ']': 1, 'howev': 1, ',': 4, 'real': 1, 'progress': 1, 'wa': 2, 'much': 1, 'slower': 1, 'alpac': 1, 'report': 1, '1966': 1, 'found': 1, 'ten-year-long': 1, 'research': 1, 'fail': 1, 'fulfil': 1, 'expect': 1, 'fund': 1, 'machin': 1, 'translat': 1, 'dramat': 1, 'reduc': 1, '.': 1}, 'Little further ': {'littl': 1, 'research': 1, 'machin': 2, 'translat': 2, 'wa': 1, 'conduct': 1, 'late': 1, '1980': 1, 'first': 1, 'statist': 1, 'system': 1, 'develop': 1, '.': 1}, 'Some notably su': {'notabl': 1, 'success': 1, 'natur': 2, 'languag': 2, 'process': 1, 'system': 2, 'develop': 1, '1960': 1, 'shrdlu': 1, ',': 4, 'work': 1, 'restrict': 2, '``': 1, 'block': 1, 'world': 1, \"''\": 1, 'vocabulari': 1, 'eliza': 1, 'simul': 1, 'rogerian': 1, 'psychotherapist': 1, 'written': 1, 'joseph': 1, 'weizenbaum': 1, '1964': 1, '1966': 1, '.': 1}, 'Using almost no': {'use': 1, 'almost': 1, 'inform': 1, 'human': 1, 'thought': 1, 'emot': 1, ',': 1, 'eliza': 1, 'sometim': 1, 'provid': 1, 'startlingli': 1, 'human-lik': 1, 'interact': 1, '.': 1}, 'When the \"patie': {'``': 4, 'patient': 1, \"''\": 2, 'exceed': 1, 'veri': 1, 'small': 1, 'knowledg': 1, 'base': 1, ',': 3, 'eliza': 1, 'might': 1, 'provid': 1, 'gener': 1, 'respons': 1, 'exampl': 1, 'respond': 1, 'head': 2, 'hurt': 2, 'whi': 1, 'say': 1, '?': 1, '.': 1}, 'During the 1970': {'dure': 1, '1970': 1, ',': 2, 'mani': 1, 'programm': 1, 'began': 1, 'write': 1, '``': 1, 'conceptu': 1, 'ontolog': 1, \"''\": 1, 'structur': 1, 'real-world': 1, 'inform': 1, 'computer-understand': 1, 'data': 1, '.': 1}, 'Examples are MA': {'exampl': 1, 'margi': 1, '(': 7, 'schank': 1, ',': 12, '1975': 1, ')': 7, 'sam': 1, 'cullingford': 1, '1978': 2, 'pam': 1, 'wilenski': 1, 'talespin': 1, 'meehan': 1, '1976': 1, 'qualm': 1, 'lehnert': 2, '1977': 1, 'polit': 1, 'carbonel': 1, '1979': 1, 'plot': 1, 'unit': 1, '1981': 1, '.': 1}, 'During this tim': {'dure': 1, 'thi': 1, 'time': 1, ',': 3, 'mani': 1, 'chatterbot': 1, 'written': 1, 'includ': 1, 'parri': 1, 'racter': 1, 'jabberwacki': 1, '.': 1}, 'Up to the 1980s': {'1980': 1, ',': 1, 'natur': 1, 'languag': 1, 'process': 1, 'system': 1, 'base': 1, 'complex': 1, 'set': 1, 'hand-written': 1, 'rule': 1, '.': 1}, 'Starting in the': {'start': 1, 'late': 1, '1980': 1, ',': 2, 'howev': 1, 'wa': 1, 'revolut': 1, 'natur': 1, 'languag': 2, 'process': 2, 'introduct': 1, 'machin': 1, 'learn': 1, 'algorithm': 1, '.': 1}, 'This was due to': {'thi': 1, 'wa': 1, 'due': 1, 'steadi': 1, 'increas': 1, 'comput': 1, 'power': 1, '(': 2, 'see': 1, 'moor': 1, \"'s\": 1, 'law': 1, ')': 1, 'gradual': 1, 'lessen': 1, 'domin': 1, 'chomskyan': 1, 'theori': 1, 'linguist': 1, 'e.g': 1, '.': 1}, 'transformationa': {'transform': 1, 'grammar': 1, ')': 1, ',': 1, 'whose': 1, 'theoret': 1, 'underpin': 1, 'discourag': 1, 'sort': 1, 'corpu': 1, 'linguist': 1, 'underli': 1, 'machine-learn': 1, 'approach': 1, 'languag': 1, 'process': 1, '.': 1}, '[3] Some of the': {'[': 1, '3': 1, ']': 1, 'earliest-us': 1, 'machin': 1, 'learn': 1, 'algorithm': 1, ',': 2, 'decis': 1, 'tree': 1, 'produc': 1, 'system': 1, 'hard': 1, 'if-then': 1, 'rule': 2, 'similar': 1, 'exist': 1, 'hand-written': 1, '.': 1}, 'However, part-o': {'howev': 1, ',': 5, 'part-of-speech': 1, 'tag': 1, 'introduc': 1, 'use': 1, 'hidden': 1, 'markov': 1, 'model': 2, 'natur': 1, 'languag': 1, 'process': 1, 'increasingli': 1, 'research': 1, 'ha': 1, 'focus': 1, 'statist': 1, 'make': 2, 'soft': 1, 'probabilist': 1, 'decis': 1, 'base': 1, 'attach': 1, 'real-valu': 1, 'weight': 1, 'featur': 1, 'input': 1, 'data': 1, '.': 1}, 'The cache langu': {'cach': 1, 'languag': 1, 'model': 2, 'upon': 1, 'mani': 1, 'speech': 1, 'recognit': 1, 'system': 1, 'reli': 1, 'exampl': 1, 'statist': 1, '.': 1}, 'Such models are': {'model': 1, 'gener': 1, 'robust': 1, 'given': 1, 'unfamiliar': 1, 'input': 2, ',': 2, 'especi': 1, 'contain': 1, 'error': 1, '(': 1, 'veri': 1, 'common': 1, 'real-world': 1, 'data': 1, ')': 1, 'produc': 1, 'reliabl': 1, 'result': 1, 'integr': 1, 'larger': 1, 'system': 1, 'compris': 1, 'multipl': 1, 'subtask': 1, '.': 1}, 'Many of the not': {'mani': 1, 'notabl': 1, 'earli': 1, 'success': 2, 'occur': 1, 'field': 1, 'machin': 1, 'translat': 1, ',': 2, 'due': 1, 'especi': 1, 'work': 1, 'ibm': 1, 'research': 1, 'complic': 1, 'statist': 1, 'model': 1, 'develop': 1, '.': 1}, 'These systems w': {'system': 2, 'abl': 1, 'take': 1, 'advantag': 1, 'exist': 1, 'multilingu': 1, 'textual': 1, 'corpora': 1, 'produc': 1, 'parliament': 1, 'canada': 1, 'european': 1, 'union': 1, 'result': 1, 'law': 1, 'call': 1, 'translat': 1, 'government': 1, 'proceed': 1, 'offici': 1, 'languag': 1, 'correspond': 1, 'govern': 1, '.': 1}, 'However, most o': {'howev': 1, ',': 2, 'system': 3, 'depend': 1, 'corpora': 1, 'specif': 1, 'develop': 1, 'task': 1, 'implement': 1, 'wa': 1, '(': 1, 'often': 1, 'continu': 1, ')': 1, 'major': 1, 'limit': 1, 'success': 1, '.': 1}, 'As a result, a ': {'result': 1, ',': 1, 'great': 1, 'deal': 1, 'research': 1, 'ha': 1, 'gone': 1, 'method': 1, 'effect': 1, 'learn': 1, 'limit': 1, 'amount': 1, 'data': 1, '.': 1}, 'Recent research': {'recent': 1, 'research': 1, 'ha': 1, 'increasingli': 1, 'focus': 1, 'unsupervis': 1, 'semi-supervis': 1, 'learn': 1, 'algorithm': 1, '.': 1}, 'Such algorithms': {'algorithm': 1, 'learn': 1, 'data': 2, 'ha': 1, 'hand-annot': 1, 'desir': 1, 'answer': 1, 'use': 1, 'combin': 1, 'annot': 1, 'non-annot': 1, '.': 1}, 'Generally, this': {'gener': 1, ',': 2, 'thi': 1, 'task': 1, 'much': 1, 'difficult': 1, 'supervis': 1, 'learn': 1, 'typic': 1, 'produc': 1, 'less': 1, 'accur': 1, 'result': 1, 'given': 1, 'amount': 1, 'input': 1, 'data': 1, '.': 1}, 'However, there ': {'howev': 1, ',': 4, 'enorm': 1, 'amount': 1, 'non-annot': 1, 'data': 1, 'avail': 1, '(': 1, 'includ': 1, 'among': 1, 'thing': 1, 'entir': 1, 'content': 1, 'world': 1, 'wide': 1, 'web': 1, ')': 1, 'often': 1, 'make': 1, 'inferior': 1, 'result': 1, 'algorithm': 1, 'use': 1, 'ha': 1, 'low': 1, 'enough': 1, 'time': 1, 'complex': 1, 'practic': 1, '.': 1}, 'In the 2010s, r': {'2010': 1, ',': 5, 'represent': 1, 'learn': 2, 'deep': 1, 'neural': 1, 'network-styl': 1, 'machin': 1, 'method': 1, 'becam': 1, 'widespread': 1, 'natur': 2, 'languag': 3, 'process': 1, 'due': 1, 'part': 1, 'flurri': 1, 'result': 2, 'show': 1, 'techniqu': 1, '[': 5, '4': 1, ']': 5, '5': 1, 'achiev': 1, 'state-of-the-art': 1, 'mani': 2, 'task': 1, 'exampl': 1, 'model': 1, '6': 1, 'pars': 1, '7': 1, '8': 1, '.': 1}, 'Popular techniq': {'popular': 1, 'techniqu': 1, 'includ': 1, 'use': 1, 'word': 2, 'embed': 1, 'captur': 1, 'semant': 1, 'properti': 1, ',': 3, 'increas': 1, 'end-to-end': 1, 'learn': 1, 'higher-level': 1, 'task': 2, '(': 2, 'e.g.': 2, 'question': 1, 'answer': 1, ')': 2, 'instead': 1, 'reli': 1, 'pipelin': 1, 'separ': 1, 'intermedi': 1, 'part-of-speech': 1, 'tag': 1, 'depend': 1, 'pars': 1, '.': 1}, 'In some areas, ': {'area': 1, ',': 2, 'thi': 1, 'shift': 1, 'ha': 1, 'entail': 1, 'substanti': 1, 'chang': 1, 'nlp': 1, 'system': 1, 'design': 1, 'deep': 1, 'neural': 1, 'network-bas': 1, 'approach': 1, 'may': 1, 'view': 1, 'new': 1, 'paradigm': 1, 'distinct': 1, 'statist': 1, 'natur': 1, 'languag': 1, 'process': 1, '.': 1}, 'For instance, t': {'instanc': 1, ',': 2, 'term': 1, 'neural': 1, 'machin': 3, 'translat': 3, '(': 2, 'nmt': 1, ')': 2, 'emphas': 1, 'fact': 1, 'deep': 1, 'learning-bas': 1, 'approach': 1, 'directli': 1, 'learn': 1, 'sequence-to-sequ': 1, 'transform': 1, 'obviat': 1, 'need': 1, 'intermedi': 1, 'step': 1, 'word': 1, 'align': 1, 'languag': 1, 'model': 1, 'wa': 1, 'use': 1, 'statist': 1, 'smt': 1, '.': 1}, 'In the early da': {'earli': 1, 'day': 1, ',': 1, 'mani': 1, 'language-process': 1, 'system': 1, 'design': 1, 'hand-cod': 1, 'set': 1, 'rule': 2, ':': 1, '[': 2, '9': 1, ']': 2, '10': 1, 'write': 1, 'grammar': 1, 'devis': 1, 'heurist': 1, 'stem': 1, '.': 1}, 'Since the so-ca': {'sinc': 1, 'so-cal': 1, '``': 1, 'statist': 1, 'revolut': 1, \"''\": 1, '[': 2, '11': 1, ']': 2, '12': 1, 'late': 1, '1980': 1, 'mid-1990': 1, ',': 1, 'much': 1, 'natur': 1, 'languag': 1, 'process': 1, 'research': 1, 'ha': 1, 'reli': 1, 'heavili': 1, 'machin': 1, 'learn': 1, '.': 1}, 'The machine-lea': {'machine-learn': 1, 'paradigm': 1, 'call': 1, 'instead': 1, 'use': 1, 'statist': 1, 'infer': 1, 'automat': 1, 'learn': 1, 'rule': 1, 'analysi': 1, 'larg': 1, 'corpora': 1, '(': 1, 'plural': 1, 'form': 1, 'corpu': 1, ',': 2, 'set': 1, 'document': 1, 'possibl': 1, 'human': 1, 'comput': 1, 'annot': 1, ')': 1, 'typic': 1, 'real-world': 1, 'exampl': 1, '.': 1}, 'Many different ': {'mani': 1, 'differ': 1, 'class': 1, 'machine-learn': 1, 'algorithm': 1, 'appli': 1, 'natural-language-process': 1, 'task': 1, '.': 1}, 'These algorithm': {'algorithm': 1, 'take': 1, 'input': 2, 'larg': 1, 'set': 1, '``': 1, 'featur': 1, \"''\": 1, 'gener': 1, 'data': 1, '.': 1}, 'Some of the ear': {'earliest-us': 1, 'algorithm': 1, ',': 2, 'decis': 1, 'tree': 1, 'produc': 1, 'system': 2, 'hard': 1, 'if-then': 1, 'rule': 2, 'similar': 1, 'handwritten': 1, 'common': 1, '.': 1}, 'Increasingly, h': {'increasingli': 1, ',': 4, 'howev': 1, 'research': 1, 'ha': 1, 'focus': 1, 'statist': 1, 'model': 1, 'make': 1, 'soft': 1, 'probabilist': 1, 'decis': 1, 'base': 1, 'attach': 1, 'real-valu': 1, 'weight': 1, 'input': 1, 'featur': 1, '.': 1}, 'Such models hav': {'model': 2, 'advantag': 1, 'express': 1, 'rel': 1, 'certainti': 1, 'mani': 1, 'differ': 1, 'possibl': 1, 'answer': 1, 'rather': 1, 'onli': 1, 'one': 1, ',': 1, 'produc': 1, 'reliabl': 1, 'result': 1, 'includ': 1, 'compon': 1, 'larger': 1, 'system': 1, '.': 1}, 'Systems based o': {'system': 1, 'base': 1, 'machine-learn': 1, 'algorithm': 1, 'mani': 1, 'advantag': 1, 'hand-produc': 1, 'rule': 1, ':': 1, 'follow': 1, 'list': 1, 'commonli': 1, 'research': 1, 'task': 1, 'natur': 1, 'languag': 1, 'process': 1, '.': 1}, 'Some of these t': {'task': 2, 'direct': 1, 'real-world': 1, 'applic': 1, ',': 1, 'commonli': 1, 'serv': 1, 'subtask': 1, 'use': 1, 'aid': 1, 'solv': 1, 'larger': 1, '.': 1}, 'Though natural ': {'though': 1, 'natur': 1, 'languag': 1, 'process': 1, 'task': 1, 'close': 1, 'intertwin': 1, ',': 1, 'frequent': 1, 'subdivid': 1, 'categori': 1, 'conveni': 1, '.': 1}, 'A coarse divisi': {'coars': 1, 'divis': 1, 'given': 1, '.': 1}, 'The first publi': {'first': 1, 'publish': 2, 'work': 1, 'artifici': 1, 'intellig': 1, 'wa': 1, '2018': 1, ',': 3, '1': 1, 'road': 1, 'market': 1, 'novel': 1, 'contain': 1, 'sixti': 1, 'million': 1, 'word': 1, '.': 1}}\n",
      "{'Natural languag': {'natur': 0.12, 'languag': 0.12, 'process': 0.08, '(': 0.08, 'nlp': 0.04, ')': 0.08, 'subfield': 0.04, 'linguist': 0.04, ',': 0.16, 'comput': 0.12, 'scienc': 0.04, 'inform': 0.04, 'engin': 0.04, 'artifici': 0.04, 'intellig': 0.04, 'concern': 0.04, 'interact': 0.04, 'human': 0.04, 'particular': 0.04, 'program': 0.04, 'analyz': 0.04, 'larg': 0.04, 'amount': 0.04, 'data': 0.04, '.': 0.04}, 'Challenges in n': {'challeng': 0.08333333333333333, 'natur': 0.25, 'languag': 0.25, 'process': 0.08333333333333333, 'frequent': 0.08333333333333333, 'involv': 0.08333333333333333, 'speech': 0.08333333333333333, 'recognit': 0.08333333333333333, ',': 0.16666666666666666, 'understand': 0.08333333333333333, 'gener': 0.08333333333333333, '.': 0.08333333333333333}, 'The history of ': {'histori': 0.058823529411764705, 'natur': 0.058823529411764705, 'languag': 0.058823529411764705, 'process': 0.058823529411764705, '(': 0.058823529411764705, 'nlp': 0.058823529411764705, ')': 0.058823529411764705, 'gener': 0.058823529411764705, 'start': 0.058823529411764705, '1950': 0.058823529411764705, ',': 0.058823529411764705, 'although': 0.058823529411764705, 'work': 0.058823529411764705, 'found': 0.058823529411764705, 'earlier': 0.058823529411764705, 'period': 0.058823529411764705, '.': 0.058823529411764705}, 'In 1950, Alan T': {'1950': 0.047619047619047616, ',': 0.047619047619047616, 'alan': 0.047619047619047616, 'ture': 0.09523809523809523, 'publish': 0.047619047619047616, 'articl': 0.047619047619047616, 'titl': 0.047619047619047616, '``': 0.047619047619047616, 'comput': 0.047619047619047616, 'machineri': 0.047619047619047616, 'intellig': 0.09523809523809523, \"''\": 0.047619047619047616, 'propos': 0.047619047619047616, 'call': 0.047619047619047616, 'test': 0.047619047619047616, 'criterion': 0.047619047619047616, '[': 0.047619047619047616, 'clarif': 0.047619047619047616, 'need': 0.047619047619047616, ']': 0.047619047619047616, '.': 0.047619047619047616}, 'The Georgetown ': {'georgetown': 0.08333333333333333, 'experi': 0.08333333333333333, '1954': 0.08333333333333333, 'involv': 0.08333333333333333, 'fulli': 0.08333333333333333, 'automat': 0.08333333333333333, 'translat': 0.08333333333333333, 'sixti': 0.08333333333333333, 'russian': 0.08333333333333333, 'sentenc': 0.08333333333333333, 'english': 0.08333333333333333, '.': 0.08333333333333333}, 'The authors cla': {'author': 0.07692307692307693, 'claim': 0.07692307692307693, 'within': 0.07692307692307693, 'three': 0.07692307692307693, 'five': 0.07692307692307693, 'year': 0.07692307692307693, ',': 0.07692307692307693, 'machin': 0.07692307692307693, 'translat': 0.07692307692307693, 'would': 0.07692307692307693, 'solv': 0.07692307692307693, 'problem': 0.07692307692307693, '.': 0.07692307692307693}, '[2]  However, r': {'[': 0.04, '2': 0.04, ']': 0.04, 'howev': 0.04, ',': 0.16, 'real': 0.04, 'progress': 0.04, 'wa': 0.08, 'much': 0.04, 'slower': 0.04, 'alpac': 0.04, 'report': 0.04, '1966': 0.04, 'found': 0.04, 'ten-year-long': 0.04, 'research': 0.04, 'fail': 0.04, 'fulfil': 0.04, 'expect': 0.04, 'fund': 0.04, 'machin': 0.04, 'translat': 0.04, 'dramat': 0.04, 'reduc': 0.04, '.': 0.04}, 'Little further ': {'littl': 0.07692307692307693, 'research': 0.07692307692307693, 'machin': 0.15384615384615385, 'translat': 0.15384615384615385, 'wa': 0.07692307692307693, 'conduct': 0.07692307692307693, 'late': 0.07692307692307693, '1980': 0.07692307692307693, 'first': 0.07692307692307693, 'statist': 0.07692307692307693, 'system': 0.07692307692307693, 'develop': 0.07692307692307693, '.': 0.07692307692307693}, 'Some notably su': {'notabl': 0.037037037037037035, 'success': 0.037037037037037035, 'natur': 0.07407407407407407, 'languag': 0.07407407407407407, 'process': 0.037037037037037035, 'system': 0.07407407407407407, 'develop': 0.037037037037037035, '1960': 0.037037037037037035, 'shrdlu': 0.037037037037037035, ',': 0.14814814814814814, 'work': 0.037037037037037035, 'restrict': 0.07407407407407407, '``': 0.037037037037037035, 'block': 0.037037037037037035, 'world': 0.037037037037037035, \"''\": 0.037037037037037035, 'vocabulari': 0.037037037037037035, 'eliza': 0.037037037037037035, 'simul': 0.037037037037037035, 'rogerian': 0.037037037037037035, 'psychotherapist': 0.037037037037037035, 'written': 0.037037037037037035, 'joseph': 0.037037037037037035, 'weizenbaum': 0.037037037037037035, '1964': 0.037037037037037035, '1966': 0.037037037037037035, '.': 0.037037037037037035}, 'Using almost no': {'use': 0.07142857142857142, 'almost': 0.07142857142857142, 'inform': 0.07142857142857142, 'human': 0.07142857142857142, 'thought': 0.07142857142857142, 'emot': 0.07142857142857142, ',': 0.07142857142857142, 'eliza': 0.07142857142857142, 'sometim': 0.07142857142857142, 'provid': 0.07142857142857142, 'startlingli': 0.07142857142857142, 'human-lik': 0.07142857142857142, 'interact': 0.07142857142857142, '.': 0.07142857142857142}, 'When the \"patie': {'``': 0.18181818181818182, 'patient': 0.045454545454545456, \"''\": 0.09090909090909091, 'exceed': 0.045454545454545456, 'veri': 0.045454545454545456, 'small': 0.045454545454545456, 'knowledg': 0.045454545454545456, 'base': 0.045454545454545456, ',': 0.13636363636363635, 'eliza': 0.045454545454545456, 'might': 0.045454545454545456, 'provid': 0.045454545454545456, 'gener': 0.045454545454545456, 'respons': 0.045454545454545456, 'exampl': 0.045454545454545456, 'respond': 0.045454545454545456, 'head': 0.09090909090909091, 'hurt': 0.09090909090909091, 'whi': 0.045454545454545456, 'say': 0.045454545454545456, '?': 0.045454545454545456, '.': 0.045454545454545456}, 'During the 1970': {'dure': 0.058823529411764705, '1970': 0.058823529411764705, ',': 0.11764705882352941, 'mani': 0.058823529411764705, 'programm': 0.058823529411764705, 'began': 0.058823529411764705, 'write': 0.058823529411764705, '``': 0.058823529411764705, 'conceptu': 0.058823529411764705, 'ontolog': 0.058823529411764705, \"''\": 0.058823529411764705, 'structur': 0.058823529411764705, 'real-world': 0.058823529411764705, 'inform': 0.058823529411764705, 'computer-understand': 0.058823529411764705, 'data': 0.058823529411764705, '.': 0.058823529411764705}, 'Examples are MA': {'exampl': 0.04, 'margi': 0.04, '(': 0.28, 'schank': 0.04, ',': 0.48, '1975': 0.04, ')': 0.28, 'sam': 0.04, 'cullingford': 0.04, '1978': 0.08, 'pam': 0.04, 'wilenski': 0.04, 'talespin': 0.04, 'meehan': 0.04, '1976': 0.04, 'qualm': 0.04, 'lehnert': 0.08, '1977': 0.04, 'polit': 0.04, 'carbonel': 0.04, '1979': 0.04, 'plot': 0.04, 'unit': 0.04, '1981': 0.04, '.': 0.04}, 'During this tim': {'dure': 0.08333333333333333, 'thi': 0.08333333333333333, 'time': 0.08333333333333333, ',': 0.25, 'mani': 0.08333333333333333, 'chatterbot': 0.08333333333333333, 'written': 0.08333333333333333, 'includ': 0.08333333333333333, 'parri': 0.08333333333333333, 'racter': 0.08333333333333333, 'jabberwacki': 0.08333333333333333, '.': 0.08333333333333333}, 'Up to the 1980s': {'1980': 0.08333333333333333, ',': 0.08333333333333333, 'natur': 0.08333333333333333, 'languag': 0.08333333333333333, 'process': 0.08333333333333333, 'system': 0.08333333333333333, 'base': 0.08333333333333333, 'complex': 0.08333333333333333, 'set': 0.08333333333333333, 'hand-written': 0.08333333333333333, 'rule': 0.08333333333333333, '.': 0.08333333333333333}, 'Starting in the': {'start': 0.06666666666666667, 'late': 0.06666666666666667, '1980': 0.06666666666666667, ',': 0.13333333333333333, 'howev': 0.06666666666666667, 'wa': 0.06666666666666667, 'revolut': 0.06666666666666667, 'natur': 0.06666666666666667, 'languag': 0.13333333333333333, 'process': 0.13333333333333333, 'introduct': 0.06666666666666667, 'machin': 0.06666666666666667, 'learn': 0.06666666666666667, 'algorithm': 0.06666666666666667, '.': 0.06666666666666667}, 'This was due to': {'thi': 0.047619047619047616, 'wa': 0.047619047619047616, 'due': 0.047619047619047616, 'steadi': 0.047619047619047616, 'increas': 0.047619047619047616, 'comput': 0.047619047619047616, 'power': 0.047619047619047616, '(': 0.09523809523809523, 'see': 0.047619047619047616, 'moor': 0.047619047619047616, \"'s\": 0.047619047619047616, 'law': 0.047619047619047616, ')': 0.047619047619047616, 'gradual': 0.047619047619047616, 'lessen': 0.047619047619047616, 'domin': 0.047619047619047616, 'chomskyan': 0.047619047619047616, 'theori': 0.047619047619047616, 'linguist': 0.047619047619047616, 'e.g': 0.047619047619047616, '.': 0.047619047619047616}, 'transformationa': {'transform': 0.058823529411764705, 'grammar': 0.058823529411764705, ')': 0.058823529411764705, ',': 0.058823529411764705, 'whose': 0.058823529411764705, 'theoret': 0.058823529411764705, 'underpin': 0.058823529411764705, 'discourag': 0.058823529411764705, 'sort': 0.058823529411764705, 'corpu': 0.058823529411764705, 'linguist': 0.058823529411764705, 'underli': 0.058823529411764705, 'machine-learn': 0.058823529411764705, 'approach': 0.058823529411764705, 'languag': 0.058823529411764705, 'process': 0.058823529411764705, '.': 0.058823529411764705}, '[3] Some of the': {'[': 0.05263157894736842, '3': 0.05263157894736842, ']': 0.05263157894736842, 'earliest-us': 0.05263157894736842, 'machin': 0.05263157894736842, 'learn': 0.05263157894736842, 'algorithm': 0.05263157894736842, ',': 0.10526315789473684, 'decis': 0.05263157894736842, 'tree': 0.05263157894736842, 'produc': 0.05263157894736842, 'system': 0.05263157894736842, 'hard': 0.05263157894736842, 'if-then': 0.05263157894736842, 'rule': 0.10526315789473684, 'similar': 0.05263157894736842, 'exist': 0.05263157894736842, 'hand-written': 0.05263157894736842, '.': 0.05263157894736842}, 'However, part-o': {'howev': 0.034482758620689655, ',': 0.1724137931034483, 'part-of-speech': 0.034482758620689655, 'tag': 0.034482758620689655, 'introduc': 0.034482758620689655, 'use': 0.034482758620689655, 'hidden': 0.034482758620689655, 'markov': 0.034482758620689655, 'model': 0.06896551724137931, 'natur': 0.034482758620689655, 'languag': 0.034482758620689655, 'process': 0.034482758620689655, 'increasingli': 0.034482758620689655, 'research': 0.034482758620689655, 'ha': 0.034482758620689655, 'focus': 0.034482758620689655, 'statist': 0.034482758620689655, 'make': 0.06896551724137931, 'soft': 0.034482758620689655, 'probabilist': 0.034482758620689655, 'decis': 0.034482758620689655, 'base': 0.034482758620689655, 'attach': 0.034482758620689655, 'real-valu': 0.034482758620689655, 'weight': 0.034482758620689655, 'featur': 0.034482758620689655, 'input': 0.034482758620689655, 'data': 0.034482758620689655, '.': 0.034482758620689655}, 'The cache langu': {'cach': 0.08333333333333333, 'languag': 0.08333333333333333, 'model': 0.16666666666666666, 'upon': 0.08333333333333333, 'mani': 0.08333333333333333, 'speech': 0.08333333333333333, 'recognit': 0.08333333333333333, 'system': 0.08333333333333333, 'reli': 0.08333333333333333, 'exampl': 0.08333333333333333, 'statist': 0.08333333333333333, '.': 0.08333333333333333}, 'Such models are': {'model': 0.038461538461538464, 'gener': 0.038461538461538464, 'robust': 0.038461538461538464, 'given': 0.038461538461538464, 'unfamiliar': 0.038461538461538464, 'input': 0.07692307692307693, ',': 0.07692307692307693, 'especi': 0.038461538461538464, 'contain': 0.038461538461538464, 'error': 0.038461538461538464, '(': 0.038461538461538464, 'veri': 0.038461538461538464, 'common': 0.038461538461538464, 'real-world': 0.038461538461538464, 'data': 0.038461538461538464, ')': 0.038461538461538464, 'produc': 0.038461538461538464, 'reliabl': 0.038461538461538464, 'result': 0.038461538461538464, 'integr': 0.038461538461538464, 'larger': 0.038461538461538464, 'system': 0.038461538461538464, 'compris': 0.038461538461538464, 'multipl': 0.038461538461538464, 'subtask': 0.038461538461538464, '.': 0.038461538461538464}, 'Many of the not': {'mani': 0.05263157894736842, 'notabl': 0.05263157894736842, 'earli': 0.05263157894736842, 'success': 0.10526315789473684, 'occur': 0.05263157894736842, 'field': 0.05263157894736842, 'machin': 0.05263157894736842, 'translat': 0.05263157894736842, ',': 0.10526315789473684, 'due': 0.05263157894736842, 'especi': 0.05263157894736842, 'work': 0.05263157894736842, 'ibm': 0.05263157894736842, 'research': 0.05263157894736842, 'complic': 0.05263157894736842, 'statist': 0.05263157894736842, 'model': 0.05263157894736842, 'develop': 0.05263157894736842, '.': 0.05263157894736842}, 'These systems w': {'system': 0.08333333333333333, 'abl': 0.041666666666666664, 'take': 0.041666666666666664, 'advantag': 0.041666666666666664, 'exist': 0.041666666666666664, 'multilingu': 0.041666666666666664, 'textual': 0.041666666666666664, 'corpora': 0.041666666666666664, 'produc': 0.041666666666666664, 'parliament': 0.041666666666666664, 'canada': 0.041666666666666664, 'european': 0.041666666666666664, 'union': 0.041666666666666664, 'result': 0.041666666666666664, 'law': 0.041666666666666664, 'call': 0.041666666666666664, 'translat': 0.041666666666666664, 'government': 0.041666666666666664, 'proceed': 0.041666666666666664, 'offici': 0.041666666666666664, 'languag': 0.041666666666666664, 'correspond': 0.041666666666666664, 'govern': 0.041666666666666664, '.': 0.041666666666666664}, 'However, most o': {'howev': 0.05555555555555555, ',': 0.1111111111111111, 'system': 0.16666666666666666, 'depend': 0.05555555555555555, 'corpora': 0.05555555555555555, 'specif': 0.05555555555555555, 'develop': 0.05555555555555555, 'task': 0.05555555555555555, 'implement': 0.05555555555555555, 'wa': 0.05555555555555555, '(': 0.05555555555555555, 'often': 0.05555555555555555, 'continu': 0.05555555555555555, ')': 0.05555555555555555, 'major': 0.05555555555555555, 'limit': 0.05555555555555555, 'success': 0.05555555555555555, '.': 0.05555555555555555}, 'As a result, a ': {'result': 0.07142857142857142, ',': 0.07142857142857142, 'great': 0.07142857142857142, 'deal': 0.07142857142857142, 'research': 0.07142857142857142, 'ha': 0.07142857142857142, 'gone': 0.07142857142857142, 'method': 0.07142857142857142, 'effect': 0.07142857142857142, 'learn': 0.07142857142857142, 'limit': 0.07142857142857142, 'amount': 0.07142857142857142, 'data': 0.07142857142857142, '.': 0.07142857142857142}, 'Recent research': {'recent': 0.1, 'research': 0.1, 'ha': 0.1, 'increasingli': 0.1, 'focus': 0.1, 'unsupervis': 0.1, 'semi-supervis': 0.1, 'learn': 0.1, 'algorithm': 0.1, '.': 0.1}, 'Such algorithms': {'algorithm': 0.08333333333333333, 'learn': 0.08333333333333333, 'data': 0.16666666666666666, 'ha': 0.08333333333333333, 'hand-annot': 0.08333333333333333, 'desir': 0.08333333333333333, 'answer': 0.08333333333333333, 'use': 0.08333333333333333, 'combin': 0.08333333333333333, 'annot': 0.08333333333333333, 'non-annot': 0.08333333333333333, '.': 0.08333333333333333}, 'Generally, this': {'gener': 0.05555555555555555, ',': 0.1111111111111111, 'thi': 0.05555555555555555, 'task': 0.05555555555555555, 'much': 0.05555555555555555, 'difficult': 0.05555555555555555, 'supervis': 0.05555555555555555, 'learn': 0.05555555555555555, 'typic': 0.05555555555555555, 'produc': 0.05555555555555555, 'less': 0.05555555555555555, 'accur': 0.05555555555555555, 'result': 0.05555555555555555, 'given': 0.05555555555555555, 'amount': 0.05555555555555555, 'input': 0.05555555555555555, 'data': 0.05555555555555555, '.': 0.05555555555555555}, 'However, there ': {'howev': 0.03333333333333333, ',': 0.13333333333333333, 'enorm': 0.03333333333333333, 'amount': 0.03333333333333333, 'non-annot': 0.03333333333333333, 'data': 0.03333333333333333, 'avail': 0.03333333333333333, '(': 0.03333333333333333, 'includ': 0.03333333333333333, 'among': 0.03333333333333333, 'thing': 0.03333333333333333, 'entir': 0.03333333333333333, 'content': 0.03333333333333333, 'world': 0.03333333333333333, 'wide': 0.03333333333333333, 'web': 0.03333333333333333, ')': 0.03333333333333333, 'often': 0.03333333333333333, 'make': 0.03333333333333333, 'inferior': 0.03333333333333333, 'result': 0.03333333333333333, 'algorithm': 0.03333333333333333, 'use': 0.03333333333333333, 'ha': 0.03333333333333333, 'low': 0.03333333333333333, 'enough': 0.03333333333333333, 'time': 0.03333333333333333, 'complex': 0.03333333333333333, 'practic': 0.03333333333333333, '.': 0.03333333333333333}, 'In the 2010s, r': {'2010': 0.02857142857142857, ',': 0.14285714285714285, 'represent': 0.02857142857142857, 'learn': 0.05714285714285714, 'deep': 0.02857142857142857, 'neural': 0.02857142857142857, 'network-styl': 0.02857142857142857, 'machin': 0.02857142857142857, 'method': 0.02857142857142857, 'becam': 0.02857142857142857, 'widespread': 0.02857142857142857, 'natur': 0.05714285714285714, 'languag': 0.08571428571428572, 'process': 0.02857142857142857, 'due': 0.02857142857142857, 'part': 0.02857142857142857, 'flurri': 0.02857142857142857, 'result': 0.05714285714285714, 'show': 0.02857142857142857, 'techniqu': 0.02857142857142857, '[': 0.14285714285714285, '4': 0.02857142857142857, ']': 0.14285714285714285, '5': 0.02857142857142857, 'achiev': 0.02857142857142857, 'state-of-the-art': 0.02857142857142857, 'mani': 0.05714285714285714, 'task': 0.02857142857142857, 'exampl': 0.02857142857142857, 'model': 0.02857142857142857, '6': 0.02857142857142857, 'pars': 0.02857142857142857, '7': 0.02857142857142857, '8': 0.02857142857142857, '.': 0.02857142857142857}, 'Popular techniq': {'popular': 0.03333333333333333, 'techniqu': 0.03333333333333333, 'includ': 0.03333333333333333, 'use': 0.03333333333333333, 'word': 0.06666666666666667, 'embed': 0.03333333333333333, 'captur': 0.03333333333333333, 'semant': 0.03333333333333333, 'properti': 0.03333333333333333, ',': 0.1, 'increas': 0.03333333333333333, 'end-to-end': 0.03333333333333333, 'learn': 0.03333333333333333, 'higher-level': 0.03333333333333333, 'task': 0.06666666666666667, '(': 0.06666666666666667, 'e.g.': 0.06666666666666667, 'question': 0.03333333333333333, 'answer': 0.03333333333333333, ')': 0.06666666666666667, 'instead': 0.03333333333333333, 'reli': 0.03333333333333333, 'pipelin': 0.03333333333333333, 'separ': 0.03333333333333333, 'intermedi': 0.03333333333333333, 'part-of-speech': 0.03333333333333333, 'tag': 0.03333333333333333, 'depend': 0.03333333333333333, 'pars': 0.03333333333333333, '.': 0.03333333333333333}, 'In some areas, ': {'area': 0.04, ',': 0.08, 'thi': 0.04, 'shift': 0.04, 'ha': 0.04, 'entail': 0.04, 'substanti': 0.04, 'chang': 0.04, 'nlp': 0.04, 'system': 0.04, 'design': 0.04, 'deep': 0.04, 'neural': 0.04, 'network-bas': 0.04, 'approach': 0.04, 'may': 0.04, 'view': 0.04, 'new': 0.04, 'paradigm': 0.04, 'distinct': 0.04, 'statist': 0.04, 'natur': 0.04, 'languag': 0.04, 'process': 0.04, '.': 0.04}, 'For instance, t': {'instanc': 0.03225806451612903, ',': 0.06451612903225806, 'term': 0.03225806451612903, 'neural': 0.03225806451612903, 'machin': 0.0967741935483871, 'translat': 0.0967741935483871, '(': 0.06451612903225806, 'nmt': 0.03225806451612903, ')': 0.06451612903225806, 'emphas': 0.03225806451612903, 'fact': 0.03225806451612903, 'deep': 0.03225806451612903, 'learning-bas': 0.03225806451612903, 'approach': 0.03225806451612903, 'directli': 0.03225806451612903, 'learn': 0.03225806451612903, 'sequence-to-sequ': 0.03225806451612903, 'transform': 0.03225806451612903, 'obviat': 0.03225806451612903, 'need': 0.03225806451612903, 'intermedi': 0.03225806451612903, 'step': 0.03225806451612903, 'word': 0.03225806451612903, 'align': 0.03225806451612903, 'languag': 0.03225806451612903, 'model': 0.03225806451612903, 'wa': 0.03225806451612903, 'use': 0.03225806451612903, 'statist': 0.03225806451612903, 'smt': 0.03225806451612903, '.': 0.03225806451612903}, 'In the early da': {'earli': 0.047619047619047616, 'day': 0.047619047619047616, ',': 0.047619047619047616, 'mani': 0.047619047619047616, 'language-process': 0.047619047619047616, 'system': 0.047619047619047616, 'design': 0.047619047619047616, 'hand-cod': 0.047619047619047616, 'set': 0.047619047619047616, 'rule': 0.09523809523809523, ':': 0.047619047619047616, '[': 0.09523809523809523, '9': 0.047619047619047616, ']': 0.09523809523809523, '10': 0.047619047619047616, 'write': 0.047619047619047616, 'grammar': 0.047619047619047616, 'devis': 0.047619047619047616, 'heurist': 0.047619047619047616, 'stem': 0.047619047619047616, '.': 0.047619047619047616}, 'Since the so-ca': {'sinc': 0.04, 'so-cal': 0.04, '``': 0.04, 'statist': 0.04, 'revolut': 0.04, \"''\": 0.04, '[': 0.08, '11': 0.04, ']': 0.08, '12': 0.04, 'late': 0.04, '1980': 0.04, 'mid-1990': 0.04, ',': 0.04, 'much': 0.04, 'natur': 0.04, 'languag': 0.04, 'process': 0.04, 'research': 0.04, 'ha': 0.04, 'reli': 0.04, 'heavili': 0.04, 'machin': 0.04, 'learn': 0.04, '.': 0.04}, 'The machine-lea': {'machine-learn': 0.034482758620689655, 'paradigm': 0.034482758620689655, 'call': 0.034482758620689655, 'instead': 0.034482758620689655, 'use': 0.034482758620689655, 'statist': 0.034482758620689655, 'infer': 0.034482758620689655, 'automat': 0.034482758620689655, 'learn': 0.034482758620689655, 'rule': 0.034482758620689655, 'analysi': 0.034482758620689655, 'larg': 0.034482758620689655, 'corpora': 0.034482758620689655, '(': 0.034482758620689655, 'plural': 0.034482758620689655, 'form': 0.034482758620689655, 'corpu': 0.034482758620689655, ',': 0.06896551724137931, 'set': 0.034482758620689655, 'document': 0.034482758620689655, 'possibl': 0.034482758620689655, 'human': 0.034482758620689655, 'comput': 0.034482758620689655, 'annot': 0.034482758620689655, ')': 0.034482758620689655, 'typic': 0.034482758620689655, 'real-world': 0.034482758620689655, 'exampl': 0.034482758620689655, '.': 0.034482758620689655}, 'Many different ': {'mani': 0.1111111111111111, 'differ': 0.1111111111111111, 'class': 0.1111111111111111, 'machine-learn': 0.1111111111111111, 'algorithm': 0.1111111111111111, 'appli': 0.1111111111111111, 'natural-language-process': 0.1111111111111111, 'task': 0.1111111111111111, '.': 0.1111111111111111}, 'These algorithm': {'algorithm': 0.09090909090909091, 'take': 0.09090909090909091, 'input': 0.18181818181818182, 'larg': 0.09090909090909091, 'set': 0.09090909090909091, '``': 0.09090909090909091, 'featur': 0.09090909090909091, \"''\": 0.09090909090909091, 'gener': 0.09090909090909091, 'data': 0.09090909090909091, '.': 0.09090909090909091}, 'Some of the ear': {'earliest-us': 0.07142857142857142, 'algorithm': 0.07142857142857142, ',': 0.14285714285714285, 'decis': 0.07142857142857142, 'tree': 0.07142857142857142, 'produc': 0.07142857142857142, 'system': 0.14285714285714285, 'hard': 0.07142857142857142, 'if-then': 0.07142857142857142, 'rule': 0.14285714285714285, 'similar': 0.07142857142857142, 'handwritten': 0.07142857142857142, 'common': 0.07142857142857142, '.': 0.07142857142857142}, 'Increasingly, h': {'increasingli': 0.05263157894736842, ',': 0.21052631578947367, 'howev': 0.05263157894736842, 'research': 0.05263157894736842, 'ha': 0.05263157894736842, 'focus': 0.05263157894736842, 'statist': 0.05263157894736842, 'model': 0.05263157894736842, 'make': 0.05263157894736842, 'soft': 0.05263157894736842, 'probabilist': 0.05263157894736842, 'decis': 0.05263157894736842, 'base': 0.05263157894736842, 'attach': 0.05263157894736842, 'real-valu': 0.05263157894736842, 'weight': 0.05263157894736842, 'input': 0.05263157894736842, 'featur': 0.05263157894736842, '.': 0.05263157894736842}, 'Such models hav': {'model': 0.09523809523809523, 'advantag': 0.047619047619047616, 'express': 0.047619047619047616, 'rel': 0.047619047619047616, 'certainti': 0.047619047619047616, 'mani': 0.047619047619047616, 'differ': 0.047619047619047616, 'possibl': 0.047619047619047616, 'answer': 0.047619047619047616, 'rather': 0.047619047619047616, 'onli': 0.047619047619047616, 'one': 0.047619047619047616, ',': 0.047619047619047616, 'produc': 0.047619047619047616, 'reliabl': 0.047619047619047616, 'result': 0.047619047619047616, 'includ': 0.047619047619047616, 'compon': 0.047619047619047616, 'larger': 0.047619047619047616, 'system': 0.047619047619047616, '.': 0.047619047619047616}, 'Systems based o': {'system': 0.05555555555555555, 'base': 0.05555555555555555, 'machine-learn': 0.05555555555555555, 'algorithm': 0.05555555555555555, 'mani': 0.05555555555555555, 'advantag': 0.05555555555555555, 'hand-produc': 0.05555555555555555, 'rule': 0.05555555555555555, ':': 0.05555555555555555, 'follow': 0.05555555555555555, 'list': 0.05555555555555555, 'commonli': 0.05555555555555555, 'research': 0.05555555555555555, 'task': 0.05555555555555555, 'natur': 0.05555555555555555, 'languag': 0.05555555555555555, 'process': 0.05555555555555555, '.': 0.05555555555555555}, 'Some of these t': {'task': 0.15384615384615385, 'direct': 0.07692307692307693, 'real-world': 0.07692307692307693, 'applic': 0.07692307692307693, ',': 0.07692307692307693, 'commonli': 0.07692307692307693, 'serv': 0.07692307692307693, 'subtask': 0.07692307692307693, 'use': 0.07692307692307693, 'aid': 0.07692307692307693, 'solv': 0.07692307692307693, 'larger': 0.07692307692307693, '.': 0.07692307692307693}, 'Though natural ': {'though': 0.07692307692307693, 'natur': 0.07692307692307693, 'languag': 0.07692307692307693, 'process': 0.07692307692307693, 'task': 0.07692307692307693, 'close': 0.07692307692307693, 'intertwin': 0.07692307692307693, ',': 0.07692307692307693, 'frequent': 0.07692307692307693, 'subdivid': 0.07692307692307693, 'categori': 0.07692307692307693, 'conveni': 0.07692307692307693, '.': 0.07692307692307693}, 'A coarse divisi': {'coars': 0.25, 'divis': 0.25, 'given': 0.25, '.': 0.25}, 'The first publi': {'first': 0.058823529411764705, 'publish': 0.11764705882352941, 'work': 0.058823529411764705, 'artifici': 0.058823529411764705, 'intellig': 0.058823529411764705, 'wa': 0.058823529411764705, '2018': 0.058823529411764705, ',': 0.17647058823529413, '1': 0.058823529411764705, 'road': 0.058823529411764705, 'market': 0.058823529411764705, 'novel': 0.058823529411764705, 'contain': 0.058823529411764705, 'sixti': 0.058823529411764705, 'million': 0.058823529411764705, 'word': 0.058823529411764705, '.': 0.058823529411764705}}\n",
      "{'natur': 12, 'languag': 16, 'process': 13, '(': 10, 'nlp': 3, ')': 11, 'subfield': 1, 'linguist': 3, ',': 36, 'comput': 4, 'scienc': 1, 'inform': 3, 'engin': 1, 'artifici': 2, 'intellig': 3, 'concern': 1, 'interact': 2, 'human': 3, 'particular': 1, 'program': 1, 'analyz': 1, 'larg': 3, 'amount': 4, 'data': 9, '.': 47, 'challeng': 1, 'frequent': 2, 'involv': 2, 'speech': 2, 'recognit': 2, 'understand': 1, 'gener': 6, 'histori': 1, 'start': 2, '1950': 2, 'although': 1, 'work': 4, 'found': 2, 'earlier': 1, 'period': 1, 'alan': 1, 'ture': 1, 'publish': 2, 'articl': 1, 'titl': 1, '``': 6, 'machineri': 1, \"''\": 6, 'propos': 1, 'call': 3, 'test': 1, 'criterion': 1, '[': 6, 'clarif': 1, 'need': 2, ']': 6, 'georgetown': 1, 'experi': 1, '1954': 1, 'fulli': 1, 'automat': 2, 'translat': 7, 'sixti': 2, 'russian': 1, 'sentenc': 1, 'english': 1, 'author': 1, 'claim': 1, 'within': 1, 'three': 1, 'five': 1, 'year': 1, 'machin': 9, 'would': 1, 'solv': 2, 'problem': 1, '2': 1, 'howev': 6, 'real': 1, 'progress': 1, 'wa': 7, 'much': 3, 'slower': 1, 'alpac': 1, 'report': 1, '1966': 2, 'ten-year-long': 1, 'research': 9, 'fail': 1, 'fulfil': 1, 'expect': 1, 'fund': 1, 'dramat': 1, 'reduc': 1, 'littl': 1, 'conduct': 1, 'late': 3, '1980': 4, 'first': 2, 'statist': 9, 'system': 13, 'develop': 4, 'notabl': 2, 'success': 3, '1960': 1, 'shrdlu': 1, 'restrict': 1, 'block': 1, 'world': 2, 'vocabulari': 1, 'eliza': 3, 'simul': 1, 'rogerian': 1, 'psychotherapist': 1, 'written': 2, 'joseph': 1, 'weizenbaum': 1, '1964': 1, 'use': 8, 'almost': 1, 'thought': 1, 'emot': 1, 'sometim': 1, 'provid': 2, 'startlingli': 1, 'human-lik': 1, 'patient': 1, 'exceed': 1, 'veri': 2, 'small': 1, 'knowledg': 1, 'base': 5, 'might': 1, 'respons': 1, 'exampl': 5, 'respond': 1, 'head': 1, 'hurt': 1, 'whi': 1, 'say': 1, '?': 1, 'dure': 2, '1970': 1, 'mani': 9, 'programm': 1, 'began': 1, 'write': 2, 'conceptu': 1, 'ontolog': 1, 'structur': 1, 'real-world': 4, 'computer-understand': 1, 'margi': 1, 'schank': 1, '1975': 1, 'sam': 1, 'cullingford': 1, '1978': 1, 'pam': 1, 'wilenski': 1, 'talespin': 1, 'meehan': 1, '1976': 1, 'qualm': 1, 'lehnert': 1, '1977': 1, 'polit': 1, 'carbonel': 1, '1979': 1, 'plot': 1, 'unit': 1, '1981': 1, 'thi': 4, 'time': 2, 'chatterbot': 1, 'includ': 4, 'parri': 1, 'racter': 1, 'jabberwacki': 1, 'complex': 2, 'set': 4, 'hand-written': 2, 'rule': 6, 'revolut': 2, 'introduct': 1, 'learn': 11, 'algorithm': 9, 'due': 3, 'steadi': 1, 'increas': 2, 'power': 1, 'see': 1, 'moor': 1, \"'s\": 1, 'law': 2, 'gradual': 1, 'lessen': 1, 'domin': 1, 'chomskyan': 1, 'theori': 1, 'e.g': 1, 'transform': 2, 'grammar': 2, 'whose': 1, 'theoret': 1, 'underpin': 1, 'discourag': 1, 'sort': 1, 'corpu': 2, 'underli': 1, 'machine-learn': 4, 'approach': 3, '3': 1, 'earliest-us': 2, 'decis': 4, 'tree': 2, 'produc': 6, 'hard': 2, 'if-then': 2, 'similar': 2, 'exist': 2, 'part-of-speech': 2, 'tag': 2, 'introduc': 1, 'hidden': 1, 'markov': 1, 'model': 8, 'increasingli': 3, 'ha': 8, 'focus': 3, 'make': 3, 'soft': 2, 'probabilist': 2, 'attach': 2, 'real-valu': 2, 'weight': 2, 'featur': 3, 'input': 5, 'cach': 1, 'upon': 1, 'reli': 3, 'robust': 1, 'given': 3, 'unfamiliar': 1, 'especi': 2, 'contain': 2, 'error': 1, 'common': 2, 'reliabl': 2, 'result': 7, 'integr': 1, 'larger': 3, 'compris': 1, 'multipl': 1, 'subtask': 2, 'earli': 2, 'occur': 1, 'field': 1, 'ibm': 1, 'complic': 1, 'abl': 1, 'take': 2, 'advantag': 3, 'multilingu': 1, 'textual': 1, 'corpora': 3, 'parliament': 1, 'canada': 1, 'european': 1, 'union': 1, 'government': 1, 'proceed': 1, 'offici': 1, 'correspond': 1, 'govern': 1, 'depend': 2, 'specif': 1, 'task': 8, 'implement': 1, 'often': 2, 'continu': 1, 'major': 1, 'limit': 2, 'great': 1, 'deal': 1, 'gone': 1, 'method': 2, 'effect': 1, 'recent': 1, 'unsupervis': 1, 'semi-supervis': 1, 'hand-annot': 1, 'desir': 1, 'answer': 3, 'combin': 1, 'annot': 2, 'non-annot': 2, 'difficult': 1, 'supervis': 1, 'typic': 2, 'less': 1, 'accur': 1, 'enorm': 1, 'avail': 1, 'among': 1, 'thing': 1, 'entir': 1, 'content': 1, 'wide': 1, 'web': 1, 'inferior': 1, 'low': 1, 'enough': 1, 'practic': 1, '2010': 1, 'represent': 1, 'deep': 3, 'neural': 3, 'network-styl': 1, 'becam': 1, 'widespread': 1, 'part': 1, 'flurri': 1, 'show': 1, 'techniqu': 2, '4': 1, '5': 1, 'achiev': 1, 'state-of-the-art': 1, '6': 1, 'pars': 2, '7': 1, '8': 1, 'popular': 1, 'word': 3, 'embed': 1, 'captur': 1, 'semant': 1, 'properti': 1, 'end-to-end': 1, 'higher-level': 1, 'e.g.': 1, 'question': 1, 'instead': 2, 'pipelin': 1, 'separ': 1, 'intermedi': 2, 'area': 1, 'shift': 1, 'entail': 1, 'substanti': 1, 'chang': 1, 'design': 2, 'network-bas': 1, 'may': 1, 'view': 1, 'new': 1, 'paradigm': 2, 'distinct': 1, 'instanc': 1, 'term': 1, 'nmt': 1, 'emphas': 1, 'fact': 1, 'learning-bas': 1, 'directli': 1, 'sequence-to-sequ': 1, 'obviat': 1, 'step': 1, 'align': 1, 'smt': 1, 'day': 1, 'language-process': 1, 'hand-cod': 1, ':': 2, '9': 1, '10': 1, 'devis': 1, 'heurist': 1, 'stem': 1, 'sinc': 1, 'so-cal': 1, '11': 1, '12': 1, 'mid-1990': 1, 'heavili': 1, 'infer': 1, 'analysi': 1, 'plural': 1, 'form': 1, 'document': 1, 'possibl': 2, 'differ': 2, 'class': 1, 'appli': 1, 'natural-language-process': 1, 'handwritten': 1, 'express': 1, 'rel': 1, 'certainti': 1, 'rather': 1, 'onli': 1, 'one': 1, 'compon': 1, 'hand-produc': 1, 'follow': 1, 'list': 1, 'commonli': 2, 'direct': 1, 'applic': 1, 'serv': 1, 'aid': 1, 'though': 1, 'close': 1, 'intertwin': 1, 'subdivid': 1, 'categori': 1, 'conveni': 1, 'coars': 1, 'divis': 1, '2018': 1, '1': 1, 'road': 1, 'market': 1, 'novel': 1, 'million': 1}\n",
      "{'Natural languag': {'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, '(': 0.6720978579357175, 'nlp': 1.194976603216055, ')': 0.6307051727774924, 'subfield': 1.6720978579357175, 'linguist': 1.194976603216055, ',': 0.1157953571684302, 'comput': 1.070037866607755, 'scienc': 1.6720978579357175, 'inform': 1.194976603216055, 'engin': 1.6720978579357175, 'artifici': 1.3710678622717363, 'intellig': 1.194976603216055, 'concern': 1.6720978579357175, 'interact': 1.3710678622717363, 'human': 1.194976603216055, 'particular': 1.6720978579357175, 'program': 1.6720978579357175, 'analyz': 1.6720978579357175, 'larg': 1.194976603216055, 'amount': 1.070037866607755, 'data': 0.7178553484963927, '.': 0.0}, 'Challenges in n': {'challeng': 1.6720978579357175, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'frequent': 1.3710678622717363, 'involv': 1.3710678622717363, 'speech': 1.3710678622717363, 'recognit': 1.3710678622717363, ',': 0.1157953571684302, 'understand': 1.6720978579357175, 'gener': 0.8939466075520738, '.': 0.0}, 'The history of ': {'histori': 1.6720978579357175, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, '(': 0.6720978579357175, 'nlp': 1.194976603216055, ')': 0.6307051727774924, 'gener': 0.8939466075520738, 'start': 1.3710678622717363, '1950': 1.3710678622717363, ',': 0.1157953571684302, 'although': 1.6720978579357175, 'work': 1.070037866607755, 'found': 1.3710678622717363, 'earlier': 1.6720978579357175, 'period': 1.6720978579357175, '.': 0.0}, 'In 1950, Alan T': {'1950': 1.3710678622717363, ',': 0.1157953571684302, 'alan': 1.6720978579357175, 'ture': 1.6720978579357175, 'publish': 1.3710678622717363, 'articl': 1.6720978579357175, 'titl': 1.6720978579357175, '``': 0.8939466075520738, 'comput': 1.070037866607755, 'machineri': 1.6720978579357175, 'intellig': 1.194976603216055, \"''\": 0.8939466075520738, 'propos': 1.6720978579357175, 'call': 1.194976603216055, 'test': 1.6720978579357175, 'criterion': 1.6720978579357175, '[': 0.8939466075520738, 'clarif': 1.6720978579357175, 'need': 1.3710678622717363, ']': 0.8939466075520738, '.': 0.0}, 'The Georgetown ': {'georgetown': 1.6720978579357175, 'experi': 1.6720978579357175, '1954': 1.6720978579357175, 'involv': 1.3710678622717363, 'fulli': 1.6720978579357175, 'automat': 1.3710678622717363, 'translat': 0.8269998179214606, 'sixti': 1.3710678622717363, 'russian': 1.6720978579357175, 'sentenc': 1.6720978579357175, 'english': 1.6720978579357175, '.': 0.0}, 'The authors cla': {'author': 1.6720978579357175, 'claim': 1.6720978579357175, 'within': 1.6720978579357175, 'three': 1.6720978579357175, 'five': 1.6720978579357175, 'year': 1.6720978579357175, ',': 0.1157953571684302, 'machin': 0.7178553484963927, 'translat': 0.8269998179214606, 'would': 1.6720978579357175, 'solv': 1.3710678622717363, 'problem': 1.6720978579357175, '.': 0.0}, '[2]  However, r': {'[': 0.8939466075520738, '2': 1.6720978579357175, ']': 0.8939466075520738, 'howev': 0.8939466075520738, ',': 0.1157953571684302, 'real': 1.6720978579357175, 'progress': 1.6720978579357175, 'wa': 0.8269998179214606, 'much': 1.194976603216055, 'slower': 1.6720978579357175, 'alpac': 1.6720978579357175, 'report': 1.6720978579357175, '1966': 1.3710678622717363, 'found': 1.3710678622717363, 'ten-year-long': 1.6720978579357175, 'research': 0.7178553484963927, 'fail': 1.6720978579357175, 'fulfil': 1.6720978579357175, 'expect': 1.6720978579357175, 'fund': 1.6720978579357175, 'machin': 0.7178553484963927, 'translat': 0.8269998179214606, 'dramat': 1.6720978579357175, 'reduc': 1.6720978579357175, '.': 0.0}, 'Little further ': {'littl': 1.6720978579357175, 'research': 0.7178553484963927, 'machin': 0.7178553484963927, 'translat': 0.8269998179214606, 'wa': 0.8269998179214606, 'conduct': 1.6720978579357175, 'late': 1.194976603216055, '1980': 1.070037866607755, 'first': 1.3710678622717363, 'statist': 0.7178553484963927, 'system': 0.5581545056288807, 'develop': 1.070037866607755, '.': 0.0}, 'Some notably su': {'notabl': 1.3710678622717363, 'success': 1.194976603216055, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'system': 0.5581545056288807, 'develop': 1.070037866607755, '1960': 1.6720978579357175, 'shrdlu': 1.6720978579357175, ',': 0.1157953571684302, 'work': 1.070037866607755, 'restrict': 1.6720978579357175, '``': 0.8939466075520738, 'block': 1.6720978579357175, 'world': 1.3710678622717363, \"''\": 0.8939466075520738, 'vocabulari': 1.6720978579357175, 'eliza': 1.194976603216055, 'simul': 1.6720978579357175, 'rogerian': 1.6720978579357175, 'psychotherapist': 1.6720978579357175, 'written': 1.3710678622717363, 'joseph': 1.6720978579357175, 'weizenbaum': 1.6720978579357175, '1964': 1.6720978579357175, '1966': 1.3710678622717363, '.': 0.0}, 'Using almost no': {'use': 0.7690078709437739, 'almost': 1.6720978579357175, 'inform': 1.194976603216055, 'human': 1.194976603216055, 'thought': 1.6720978579357175, 'emot': 1.6720978579357175, ',': 0.1157953571684302, 'eliza': 1.194976603216055, 'sometim': 1.6720978579357175, 'provid': 1.3710678622717363, 'startlingli': 1.6720978579357175, 'human-lik': 1.6720978579357175, 'interact': 1.3710678622717363, '.': 0.0}, 'When the \"patie': {'``': 0.8939466075520738, 'patient': 1.6720978579357175, \"''\": 0.8939466075520738, 'exceed': 1.6720978579357175, 'veri': 1.3710678622717363, 'small': 1.6720978579357175, 'knowledg': 1.6720978579357175, 'base': 0.9731278535996987, ',': 0.1157953571684302, 'eliza': 1.194976603216055, 'might': 1.6720978579357175, 'provid': 1.3710678622717363, 'gener': 0.8939466075520738, 'respons': 1.6720978579357175, 'exampl': 0.9731278535996987, 'respond': 1.6720978579357175, 'head': 1.6720978579357175, 'hurt': 1.6720978579357175, 'whi': 1.6720978579357175, 'say': 1.6720978579357175, '?': 1.6720978579357175, '.': 0.0}, 'During the 1970': {'dure': 1.3710678622717363, '1970': 1.6720978579357175, ',': 0.1157953571684302, 'mani': 0.7178553484963927, 'programm': 1.6720978579357175, 'began': 1.6720978579357175, 'write': 1.3710678622717363, '``': 0.8939466075520738, 'conceptu': 1.6720978579357175, 'ontolog': 1.6720978579357175, \"''\": 0.8939466075520738, 'structur': 1.6720978579357175, 'real-world': 1.070037866607755, 'inform': 1.194976603216055, 'computer-understand': 1.6720978579357175, 'data': 0.7178553484963927, '.': 0.0}, 'Examples are MA': {'exampl': 0.9731278535996987, 'margi': 1.6720978579357175, '(': 0.6720978579357175, 'schank': 1.6720978579357175, ',': 0.1157953571684302, '1975': 1.6720978579357175, ')': 0.6307051727774924, 'sam': 1.6720978579357175, 'cullingford': 1.6720978579357175, '1978': 1.6720978579357175, 'pam': 1.6720978579357175, 'wilenski': 1.6720978579357175, 'talespin': 1.6720978579357175, 'meehan': 1.6720978579357175, '1976': 1.6720978579357175, 'qualm': 1.6720978579357175, 'lehnert': 1.6720978579357175, '1977': 1.6720978579357175, 'polit': 1.6720978579357175, 'carbonel': 1.6720978579357175, '1979': 1.6720978579357175, 'plot': 1.6720978579357175, 'unit': 1.6720978579357175, '1981': 1.6720978579357175, '.': 0.0}, 'During this tim': {'dure': 1.3710678622717363, 'thi': 1.070037866607755, 'time': 1.3710678622717363, ',': 0.1157953571684302, 'mani': 0.7178553484963927, 'chatterbot': 1.6720978579357175, 'written': 1.3710678622717363, 'includ': 1.070037866607755, 'parri': 1.6720978579357175, 'racter': 1.6720978579357175, 'jabberwacki': 1.6720978579357175, '.': 0.0}, 'Up to the 1980s': {'1980': 1.070037866607755, ',': 0.1157953571684302, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'system': 0.5581545056288807, 'base': 0.9731278535996987, 'complex': 1.3710678622717363, 'set': 1.070037866607755, 'hand-written': 1.3710678622717363, 'rule': 0.8939466075520738, '.': 0.0}, 'Starting in the': {'start': 1.3710678622717363, 'late': 1.194976603216055, '1980': 1.070037866607755, ',': 0.1157953571684302, 'howev': 0.8939466075520738, 'wa': 0.8269998179214606, 'revolut': 1.3710678622717363, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'introduct': 1.6720978579357175, 'machin': 0.7178553484963927, 'learn': 0.6307051727774924, 'algorithm': 0.7178553484963927, '.': 0.0}, 'This was due to': {'thi': 1.070037866607755, 'wa': 0.8269998179214606, 'due': 1.194976603216055, 'steadi': 1.6720978579357175, 'increas': 1.3710678622717363, 'comput': 1.070037866607755, 'power': 1.6720978579357175, '(': 0.6720978579357175, 'see': 1.6720978579357175, 'moor': 1.6720978579357175, \"'s\": 1.6720978579357175, 'law': 1.3710678622717363, ')': 0.6307051727774924, 'gradual': 1.6720978579357175, 'lessen': 1.6720978579357175, 'domin': 1.6720978579357175, 'chomskyan': 1.6720978579357175, 'theori': 1.6720978579357175, 'linguist': 1.194976603216055, 'e.g': 1.6720978579357175, '.': 0.0}, 'transformationa': {'transform': 1.3710678622717363, 'grammar': 1.3710678622717363, ')': 0.6307051727774924, ',': 0.1157953571684302, 'whose': 1.6720978579357175, 'theoret': 1.6720978579357175, 'underpin': 1.6720978579357175, 'discourag': 1.6720978579357175, 'sort': 1.6720978579357175, 'corpu': 1.3710678622717363, 'linguist': 1.194976603216055, 'underli': 1.6720978579357175, 'machine-learn': 1.070037866607755, 'approach': 1.194976603216055, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, '.': 0.0}, '[3] Some of the': {'[': 0.8939466075520738, '3': 1.6720978579357175, ']': 0.8939466075520738, 'earliest-us': 1.3710678622717363, 'machin': 0.7178553484963927, 'learn': 0.6307051727774924, 'algorithm': 0.7178553484963927, ',': 0.1157953571684302, 'decis': 1.070037866607755, 'tree': 1.3710678622717363, 'produc': 0.8939466075520738, 'system': 0.5581545056288807, 'hard': 1.3710678622717363, 'if-then': 1.3710678622717363, 'rule': 0.8939466075520738, 'similar': 1.3710678622717363, 'exist': 1.3710678622717363, 'hand-written': 1.3710678622717363, '.': 0.0}, 'However, part-o': {'howev': 0.8939466075520738, ',': 0.1157953571684302, 'part-of-speech': 1.3710678622717363, 'tag': 1.3710678622717363, 'introduc': 1.6720978579357175, 'use': 0.7690078709437739, 'hidden': 1.6720978579357175, 'markov': 1.6720978579357175, 'model': 0.7690078709437739, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'increasingli': 1.194976603216055, 'research': 0.7178553484963927, 'ha': 0.7690078709437739, 'focus': 1.194976603216055, 'statist': 0.7178553484963927, 'make': 1.194976603216055, 'soft': 1.3710678622717363, 'probabilist': 1.3710678622717363, 'decis': 1.070037866607755, 'base': 0.9731278535996987, 'attach': 1.3710678622717363, 'real-valu': 1.3710678622717363, 'weight': 1.3710678622717363, 'featur': 1.194976603216055, 'input': 0.9731278535996987, 'data': 0.7178553484963927, '.': 0.0}, 'The cache langu': {'cach': 1.6720978579357175, 'languag': 0.4679778752797927, 'model': 0.7690078709437739, 'upon': 1.6720978579357175, 'mani': 0.7178553484963927, 'speech': 1.3710678622717363, 'recognit': 1.3710678622717363, 'system': 0.5581545056288807, 'reli': 1.194976603216055, 'exampl': 0.9731278535996987, 'statist': 0.7178553484963927, '.': 0.0}, 'Such models are': {'model': 0.7690078709437739, 'gener': 0.8939466075520738, 'robust': 1.6720978579357175, 'given': 1.194976603216055, 'unfamiliar': 1.6720978579357175, 'input': 0.9731278535996987, ',': 0.1157953571684302, 'especi': 1.3710678622717363, 'contain': 1.3710678622717363, 'error': 1.6720978579357175, '(': 0.6720978579357175, 'veri': 1.3710678622717363, 'common': 1.3710678622717363, 'real-world': 1.070037866607755, 'data': 0.7178553484963927, ')': 0.6307051727774924, 'produc': 0.8939466075520738, 'reliabl': 1.3710678622717363, 'result': 0.8269998179214606, 'integr': 1.6720978579357175, 'larger': 1.194976603216055, 'system': 0.5581545056288807, 'compris': 1.6720978579357175, 'multipl': 1.6720978579357175, 'subtask': 1.3710678622717363, '.': 0.0}, 'Many of the not': {'mani': 0.7178553484963927, 'notabl': 1.3710678622717363, 'earli': 1.3710678622717363, 'success': 1.194976603216055, 'occur': 1.6720978579357175, 'field': 1.6720978579357175, 'machin': 0.7178553484963927, 'translat': 0.8269998179214606, ',': 0.1157953571684302, 'due': 1.194976603216055, 'especi': 1.3710678622717363, 'work': 1.070037866607755, 'ibm': 1.6720978579357175, 'research': 0.7178553484963927, 'complic': 1.6720978579357175, 'statist': 0.7178553484963927, 'model': 0.7690078709437739, 'develop': 1.070037866607755, '.': 0.0}, 'These systems w': {'system': 0.5581545056288807, 'abl': 1.6720978579357175, 'take': 1.3710678622717363, 'advantag': 1.194976603216055, 'exist': 1.3710678622717363, 'multilingu': 1.6720978579357175, 'textual': 1.6720978579357175, 'corpora': 1.194976603216055, 'produc': 0.8939466075520738, 'parliament': 1.6720978579357175, 'canada': 1.6720978579357175, 'european': 1.6720978579357175, 'union': 1.6720978579357175, 'result': 0.8269998179214606, 'law': 1.3710678622717363, 'call': 1.194976603216055, 'translat': 0.8269998179214606, 'government': 1.6720978579357175, 'proceed': 1.6720978579357175, 'offici': 1.6720978579357175, 'languag': 0.4679778752797927, 'correspond': 1.6720978579357175, 'govern': 1.6720978579357175, '.': 0.0}, 'However, most o': {'howev': 0.8939466075520738, ',': 0.1157953571684302, 'system': 0.5581545056288807, 'depend': 1.3710678622717363, 'corpora': 1.194976603216055, 'specif': 1.6720978579357175, 'develop': 1.070037866607755, 'task': 0.7690078709437739, 'implement': 1.6720978579357175, 'wa': 0.8269998179214606, '(': 0.6720978579357175, 'often': 1.3710678622717363, 'continu': 1.6720978579357175, ')': 0.6307051727774924, 'major': 1.6720978579357175, 'limit': 1.3710678622717363, 'success': 1.194976603216055, '.': 0.0}, 'As a result, a ': {'result': 0.8269998179214606, ',': 0.1157953571684302, 'great': 1.6720978579357175, 'deal': 1.6720978579357175, 'research': 0.7178553484963927, 'ha': 0.7690078709437739, 'gone': 1.6720978579357175, 'method': 1.3710678622717363, 'effect': 1.6720978579357175, 'learn': 0.6307051727774924, 'limit': 1.3710678622717363, 'amount': 1.070037866607755, 'data': 0.7178553484963927, '.': 0.0}, 'Recent research': {'recent': 1.6720978579357175, 'research': 0.7178553484963927, 'ha': 0.7690078709437739, 'increasingli': 1.194976603216055, 'focus': 1.194976603216055, 'unsupervis': 1.6720978579357175, 'semi-supervis': 1.6720978579357175, 'learn': 0.6307051727774924, 'algorithm': 0.7178553484963927, '.': 0.0}, 'Such algorithms': {'algorithm': 0.7178553484963927, 'learn': 0.6307051727774924, 'data': 0.7178553484963927, 'ha': 0.7690078709437739, 'hand-annot': 1.6720978579357175, 'desir': 1.6720978579357175, 'answer': 1.194976603216055, 'use': 0.7690078709437739, 'combin': 1.6720978579357175, 'annot': 1.3710678622717363, 'non-annot': 1.3710678622717363, '.': 0.0}, 'Generally, this': {'gener': 0.8939466075520738, ',': 0.1157953571684302, 'thi': 1.070037866607755, 'task': 0.7690078709437739, 'much': 1.194976603216055, 'difficult': 1.6720978579357175, 'supervis': 1.6720978579357175, 'learn': 0.6307051727774924, 'typic': 1.3710678622717363, 'produc': 0.8939466075520738, 'less': 1.6720978579357175, 'accur': 1.6720978579357175, 'result': 0.8269998179214606, 'given': 1.194976603216055, 'amount': 1.070037866607755, 'input': 0.9731278535996987, 'data': 0.7178553484963927, '.': 0.0}, 'However, there ': {'howev': 0.8939466075520738, ',': 0.1157953571684302, 'enorm': 1.6720978579357175, 'amount': 1.070037866607755, 'non-annot': 1.3710678622717363, 'data': 0.7178553484963927, 'avail': 1.6720978579357175, '(': 0.6720978579357175, 'includ': 1.070037866607755, 'among': 1.6720978579357175, 'thing': 1.6720978579357175, 'entir': 1.6720978579357175, 'content': 1.6720978579357175, 'world': 1.3710678622717363, 'wide': 1.6720978579357175, 'web': 1.6720978579357175, ')': 0.6307051727774924, 'often': 1.3710678622717363, 'make': 1.194976603216055, 'inferior': 1.6720978579357175, 'result': 0.8269998179214606, 'algorithm': 0.7178553484963927, 'use': 0.7690078709437739, 'ha': 0.7690078709437739, 'low': 1.6720978579357175, 'enough': 1.6720978579357175, 'time': 1.3710678622717363, 'complex': 1.3710678622717363, 'practic': 1.6720978579357175, '.': 0.0}, 'In the 2010s, r': {'2010': 1.6720978579357175, ',': 0.1157953571684302, 'represent': 1.6720978579357175, 'learn': 0.6307051727774924, 'deep': 1.194976603216055, 'neural': 1.194976603216055, 'network-styl': 1.6720978579357175, 'machin': 0.7178553484963927, 'method': 1.3710678622717363, 'becam': 1.6720978579357175, 'widespread': 1.6720978579357175, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'due': 1.194976603216055, 'part': 1.6720978579357175, 'flurri': 1.6720978579357175, 'result': 0.8269998179214606, 'show': 1.6720978579357175, 'techniqu': 1.3710678622717363, '[': 0.8939466075520738, '4': 1.6720978579357175, ']': 0.8939466075520738, '5': 1.6720978579357175, 'achiev': 1.6720978579357175, 'state-of-the-art': 1.6720978579357175, 'mani': 0.7178553484963927, 'task': 0.7690078709437739, 'exampl': 0.9731278535996987, 'model': 0.7690078709437739, '6': 1.6720978579357175, 'pars': 1.3710678622717363, '7': 1.6720978579357175, '8': 1.6720978579357175, '.': 0.0}, 'Popular techniq': {'popular': 1.6720978579357175, 'techniqu': 1.3710678622717363, 'includ': 1.070037866607755, 'use': 0.7690078709437739, 'word': 1.194976603216055, 'embed': 1.6720978579357175, 'captur': 1.6720978579357175, 'semant': 1.6720978579357175, 'properti': 1.6720978579357175, ',': 0.1157953571684302, 'increas': 1.3710678622717363, 'end-to-end': 1.6720978579357175, 'learn': 0.6307051727774924, 'higher-level': 1.6720978579357175, 'task': 0.7690078709437739, '(': 0.6720978579357175, 'e.g.': 1.6720978579357175, 'question': 1.6720978579357175, 'answer': 1.194976603216055, ')': 0.6307051727774924, 'instead': 1.3710678622717363, 'reli': 1.194976603216055, 'pipelin': 1.6720978579357175, 'separ': 1.6720978579357175, 'intermedi': 1.3710678622717363, 'part-of-speech': 1.3710678622717363, 'tag': 1.3710678622717363, 'depend': 1.3710678622717363, 'pars': 1.3710678622717363, '.': 0.0}, 'In some areas, ': {'area': 1.6720978579357175, ',': 0.1157953571684302, 'thi': 1.070037866607755, 'shift': 1.6720978579357175, 'ha': 0.7690078709437739, 'entail': 1.6720978579357175, 'substanti': 1.6720978579357175, 'chang': 1.6720978579357175, 'nlp': 1.194976603216055, 'system': 0.5581545056288807, 'design': 1.3710678622717363, 'deep': 1.194976603216055, 'neural': 1.194976603216055, 'network-bas': 1.6720978579357175, 'approach': 1.194976603216055, 'may': 1.6720978579357175, 'view': 1.6720978579357175, 'new': 1.6720978579357175, 'paradigm': 1.3710678622717363, 'distinct': 1.6720978579357175, 'statist': 0.7178553484963927, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, '.': 0.0}, 'For instance, t': {'instanc': 1.6720978579357175, ',': 0.1157953571684302, 'term': 1.6720978579357175, 'neural': 1.194976603216055, 'machin': 0.7178553484963927, 'translat': 0.8269998179214606, '(': 0.6720978579357175, 'nmt': 1.6720978579357175, ')': 0.6307051727774924, 'emphas': 1.6720978579357175, 'fact': 1.6720978579357175, 'deep': 1.194976603216055, 'learning-bas': 1.6720978579357175, 'approach': 1.194976603216055, 'directli': 1.6720978579357175, 'learn': 0.6307051727774924, 'sequence-to-sequ': 1.6720978579357175, 'transform': 1.3710678622717363, 'obviat': 1.6720978579357175, 'need': 1.3710678622717363, 'intermedi': 1.3710678622717363, 'step': 1.6720978579357175, 'word': 1.194976603216055, 'align': 1.6720978579357175, 'languag': 0.4679778752797927, 'model': 0.7690078709437739, 'wa': 0.8269998179214606, 'use': 0.7690078709437739, 'statist': 0.7178553484963927, 'smt': 1.6720978579357175, '.': 0.0}, 'In the early da': {'earli': 1.3710678622717363, 'day': 1.6720978579357175, ',': 0.1157953571684302, 'mani': 0.7178553484963927, 'language-process': 1.6720978579357175, 'system': 0.5581545056288807, 'design': 1.3710678622717363, 'hand-cod': 1.6720978579357175, 'set': 1.070037866607755, 'rule': 0.8939466075520738, ':': 1.3710678622717363, '[': 0.8939466075520738, '9': 1.6720978579357175, ']': 0.8939466075520738, '10': 1.6720978579357175, 'write': 1.3710678622717363, 'grammar': 1.3710678622717363, 'devis': 1.6720978579357175, 'heurist': 1.6720978579357175, 'stem': 1.6720978579357175, '.': 0.0}, 'Since the so-ca': {'sinc': 1.6720978579357175, 'so-cal': 1.6720978579357175, '``': 0.8939466075520738, 'statist': 0.7178553484963927, 'revolut': 1.3710678622717363, \"''\": 0.8939466075520738, '[': 0.8939466075520738, '11': 1.6720978579357175, ']': 0.8939466075520738, '12': 1.6720978579357175, 'late': 1.194976603216055, '1980': 1.070037866607755, 'mid-1990': 1.6720978579357175, ',': 0.1157953571684302, 'much': 1.194976603216055, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'research': 0.7178553484963927, 'ha': 0.7690078709437739, 'reli': 1.194976603216055, 'heavili': 1.6720978579357175, 'machin': 0.7178553484963927, 'learn': 0.6307051727774924, '.': 0.0}, 'The machine-lea': {'machine-learn': 1.070037866607755, 'paradigm': 1.3710678622717363, 'call': 1.194976603216055, 'instead': 1.3710678622717363, 'use': 0.7690078709437739, 'statist': 0.7178553484963927, 'infer': 1.6720978579357175, 'automat': 1.3710678622717363, 'learn': 0.6307051727774924, 'rule': 0.8939466075520738, 'analysi': 1.6720978579357175, 'larg': 1.194976603216055, 'corpora': 1.194976603216055, '(': 0.6720978579357175, 'plural': 1.6720978579357175, 'form': 1.6720978579357175, 'corpu': 1.3710678622717363, ',': 0.1157953571684302, 'set': 1.070037866607755, 'document': 1.6720978579357175, 'possibl': 1.3710678622717363, 'human': 1.194976603216055, 'comput': 1.070037866607755, 'annot': 1.3710678622717363, ')': 0.6307051727774924, 'typic': 1.3710678622717363, 'real-world': 1.070037866607755, 'exampl': 0.9731278535996987, '.': 0.0}, 'Many different ': {'mani': 0.7178553484963927, 'differ': 1.3710678622717363, 'class': 1.6720978579357175, 'machine-learn': 1.070037866607755, 'algorithm': 0.7178553484963927, 'appli': 1.6720978579357175, 'natural-language-process': 1.6720978579357175, 'task': 0.7690078709437739, '.': 0.0}, 'These algorithm': {'algorithm': 0.7178553484963927, 'take': 1.3710678622717363, 'input': 0.9731278535996987, 'larg': 1.194976603216055, 'set': 1.070037866607755, '``': 0.8939466075520738, 'featur': 1.194976603216055, \"''\": 0.8939466075520738, 'gener': 0.8939466075520738, 'data': 0.7178553484963927, '.': 0.0}, 'Some of the ear': {'earliest-us': 1.3710678622717363, 'algorithm': 0.7178553484963927, ',': 0.1157953571684302, 'decis': 1.070037866607755, 'tree': 1.3710678622717363, 'produc': 0.8939466075520738, 'system': 0.5581545056288807, 'hard': 1.3710678622717363, 'if-then': 1.3710678622717363, 'rule': 0.8939466075520738, 'similar': 1.3710678622717363, 'handwritten': 1.6720978579357175, 'common': 1.3710678622717363, '.': 0.0}, 'Increasingly, h': {'increasingli': 1.194976603216055, ',': 0.1157953571684302, 'howev': 0.8939466075520738, 'research': 0.7178553484963927, 'ha': 0.7690078709437739, 'focus': 1.194976603216055, 'statist': 0.7178553484963927, 'model': 0.7690078709437739, 'make': 1.194976603216055, 'soft': 1.3710678622717363, 'probabilist': 1.3710678622717363, 'decis': 1.070037866607755, 'base': 0.9731278535996987, 'attach': 1.3710678622717363, 'real-valu': 1.3710678622717363, 'weight': 1.3710678622717363, 'input': 0.9731278535996987, 'featur': 1.194976603216055, '.': 0.0}, 'Such models hav': {'model': 0.7690078709437739, 'advantag': 1.194976603216055, 'express': 1.6720978579357175, 'rel': 1.6720978579357175, 'certainti': 1.6720978579357175, 'mani': 0.7178553484963927, 'differ': 1.3710678622717363, 'possibl': 1.3710678622717363, 'answer': 1.194976603216055, 'rather': 1.6720978579357175, 'onli': 1.6720978579357175, 'one': 1.6720978579357175, ',': 0.1157953571684302, 'produc': 0.8939466075520738, 'reliabl': 1.3710678622717363, 'result': 0.8269998179214606, 'includ': 1.070037866607755, 'compon': 1.6720978579357175, 'larger': 1.194976603216055, 'system': 0.5581545056288807, '.': 0.0}, 'Systems based o': {'system': 0.5581545056288807, 'base': 0.9731278535996987, 'machine-learn': 1.070037866607755, 'algorithm': 0.7178553484963927, 'mani': 0.7178553484963927, 'advantag': 1.194976603216055, 'hand-produc': 1.6720978579357175, 'rule': 0.8939466075520738, ':': 1.3710678622717363, 'follow': 1.6720978579357175, 'list': 1.6720978579357175, 'commonli': 1.3710678622717363, 'research': 0.7178553484963927, 'task': 0.7690078709437739, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, '.': 0.0}, 'Some of these t': {'task': 0.7690078709437739, 'direct': 1.6720978579357175, 'real-world': 1.070037866607755, 'applic': 1.6720978579357175, ',': 0.1157953571684302, 'commonli': 1.3710678622717363, 'serv': 1.6720978579357175, 'subtask': 1.3710678622717363, 'use': 0.7690078709437739, 'aid': 1.6720978579357175, 'solv': 1.3710678622717363, 'larger': 1.194976603216055, '.': 0.0}, 'Though natural ': {'though': 1.6720978579357175, 'natur': 0.5929166118880926, 'languag': 0.4679778752797927, 'process': 0.5581545056288807, 'task': 0.7690078709437739, 'close': 1.6720978579357175, 'intertwin': 1.6720978579357175, ',': 0.1157953571684302, 'frequent': 1.3710678622717363, 'subdivid': 1.6720978579357175, 'categori': 1.6720978579357175, 'conveni': 1.6720978579357175, '.': 0.0}, 'A coarse divisi': {'coars': 1.6720978579357175, 'divis': 1.6720978579357175, 'given': 1.194976603216055, '.': 0.0}, 'The first publi': {'first': 1.3710678622717363, 'publish': 1.3710678622717363, 'work': 1.070037866607755, 'artifici': 1.3710678622717363, 'intellig': 1.194976603216055, 'wa': 0.8269998179214606, '2018': 1.6720978579357175, ',': 0.1157953571684302, '1': 1.6720978579357175, 'road': 1.6720978579357175, 'market': 1.6720978579357175, 'novel': 1.6720978579357175, 'contain': 1.3710678622717363, 'sixti': 1.3710678622717363, 'million': 1.6720978579357175, 'word': 1.194976603216055, '.': 0.0}}\n",
      "{'Natural languag': {'natur': 0.07114999342657112, 'languag': 0.05615734503357512, 'process': 0.04465236045031046, '(': 0.053767828634857406, 'nlp': 0.047799064128642196, ')': 0.05045641382219939, 'subfield': 0.0668839143174287, 'linguist': 0.047799064128642196, ',': 0.018527257146948833, 'comput': 0.1284045439929306, 'scienc': 0.0668839143174287, 'inform': 0.047799064128642196, 'engin': 0.0668839143174287, 'artifici': 0.05484271449086945, 'intellig': 0.047799064128642196, 'concern': 0.0668839143174287, 'interact': 0.05484271449086945, 'human': 0.047799064128642196, 'particular': 0.0668839143174287, 'program': 0.0668839143174287, 'analyz': 0.0668839143174287, 'larg': 0.047799064128642196, 'amount': 0.0428015146643102, 'data': 0.028714213939855706, '.': 0.0}, 'Challenges in n': {'challeng': 0.13934148816130978, 'natur': 0.14822915297202316, 'languag': 0.11699446881994817, 'process': 0.046512875469073386, 'frequent': 0.11425565518931136, 'involv': 0.11425565518931136, 'speech': 0.11425565518931136, 'recognit': 0.11425565518931136, ',': 0.019299226194738367, 'understand': 0.13934148816130978, 'gener': 0.07449555062933948, '.': 0.0}, 'The history of ': {'histori': 0.09835869752563044, 'natur': 0.0348774477581231, 'languag': 0.02752811031057604, 'process': 0.03283261797816945, '(': 0.03953516811386574, 'nlp': 0.0702927413656503, ')': 0.037100304281028965, 'gener': 0.05258509456188669, 'start': 0.08065105072186683, '1950': 0.08065105072186683, ',': 0.006811491598142953, 'although': 0.09835869752563044, 'work': 0.06294340391810324, 'found': 0.08065105072186683, 'earlier': 0.09835869752563044, 'period': 0.09835869752563044, '.': 0.0}, 'In 1950, Alan T': {'1950': 0.06528894582246363, ',': 0.005514064627068105, 'alan': 0.07962370752074845, 'ture': 0.1592474150414969, 'publish': 0.06528894582246363, 'articl': 0.07962370752074845, 'titl': 0.07962370752074845, '``': 0.042568886073908276, 'comput': 0.050954184124178806, 'machineri': 0.07962370752074845, 'intellig': 0.11380729554438618, \"''\": 0.042568886073908276, 'propos': 0.07962370752074845, 'call': 0.05690364777219309, 'test': 0.07962370752074845, 'criterion': 0.07962370752074845, '[': 0.042568886073908276, 'clarif': 0.07962370752074845, 'need': 0.06528894582246363, ']': 0.042568886073908276, '.': 0.0}, 'The Georgetown ': {'georgetown': 0.13934148816130978, 'experi': 0.13934148816130978, '1954': 0.13934148816130978, 'involv': 0.11425565518931136, 'fulli': 0.13934148816130978, 'automat': 0.11425565518931136, 'translat': 0.06891665149345505, 'sixti': 0.11425565518931136, 'russian': 0.13934148816130978, 'sentenc': 0.13934148816130978, 'english': 0.13934148816130978, '.': 0.0}, 'The authors cla': {'author': 0.12862291214890134, 'claim': 0.12862291214890134, 'within': 0.12862291214890134, 'three': 0.12862291214890134, 'five': 0.12862291214890134, 'year': 0.12862291214890134, ',': 0.008907335166802324, 'machin': 0.05521964219203021, 'translat': 0.06361537060934312, 'would': 0.12862291214890134, 'solv': 0.1054667586362874, 'problem': 0.12862291214890134, '.': 0.0}, '[2]  However, r': {'[': 0.03575786430208295, '2': 0.0668839143174287, ']': 0.03575786430208295, 'howev': 0.03575786430208295, ',': 0.018527257146948833, 'real': 0.0668839143174287, 'progress': 0.0668839143174287, 'wa': 0.06615998543371684, 'much': 0.047799064128642196, 'slower': 0.0668839143174287, 'alpac': 0.0668839143174287, 'report': 0.0668839143174287, '1966': 0.05484271449086945, 'found': 0.05484271449086945, 'ten-year-long': 0.0668839143174287, 'research': 0.028714213939855706, 'fail': 0.0668839143174287, 'fulfil': 0.0668839143174287, 'expect': 0.0668839143174287, 'fund': 0.0668839143174287, 'machin': 0.028714213939855706, 'translat': 0.03307999271685842, 'dramat': 0.0668839143174287, 'reduc': 0.0668839143174287, '.': 0.0}, 'Little further ': {'littl': 0.12862291214890134, 'research': 0.05521964219203021, 'machin': 0.11043928438406042, 'translat': 0.12723074121868624, 'wa': 0.06361537060934312, 'conduct': 0.12862291214890134, 'late': 0.09192127717046576, '1980': 0.08231060512367347, 'first': 0.1054667586362874, 'statist': 0.05521964219203021, 'system': 0.042934961971452364, 'develop': 0.08231060512367347, '.': 0.0}, 'Some notably su': {'notabl': 0.05078029119524949, 'success': 0.04425839271170574, 'natur': 0.0439197490287476, 'languag': 0.03466502779850316, 'process': 0.02067238909736595, 'system': 0.0413447781947319, 'develop': 0.039631032096583516, '1960': 0.06192955029391546, 'shrdlu': 0.06192955029391546, ',': 0.017154867728656327, 'work': 0.039631032096583516, 'restrict': 0.12385910058783092, '``': 0.03310913361303977, 'block': 0.06192955029391546, 'world': 0.05078029119524949, \"''\": 0.03310913361303977, 'vocabulari': 0.06192955029391546, 'eliza': 0.04425839271170574, 'simul': 0.06192955029391546, 'rogerian': 0.06192955029391546, 'psychotherapist': 0.06192955029391546, 'written': 0.05078029119524949, 'joseph': 0.06192955029391546, 'weizenbaum': 0.06192955029391546, '1964': 0.06192955029391546, '1966': 0.05078029119524949, '.': 0.0}, 'Using almost no': {'use': 0.054929133638840985, 'almost': 0.11943556128112268, 'inform': 0.08535547165828963, 'human': 0.08535547165828963, 'thought': 0.11943556128112268, 'emot': 0.11943556128112268, ',': 0.008271096940602157, 'eliza': 0.08535547165828963, 'sometim': 0.11943556128112268, 'provid': 0.09793341873369545, 'startlingli': 0.11943556128112268, 'human-lik': 0.11943556128112268, 'interact': 0.09793341873369545, '.': 0.0}, 'When the \"patie': {'``': 0.1625357468276498, 'patient': 0.07600444808798716, \"''\": 0.0812678734138249, 'exceed': 0.07600444808798716, 'veri': 0.062321266466897104, 'small': 0.07600444808798716, 'knowledg': 0.07600444808798716, 'base': 0.044233084254531756, ',': 0.01579027597751321, 'eliza': 0.0543171183280025, 'might': 0.07600444808798716, 'provid': 0.062321266466897104, 'gener': 0.04063393670691245, 'respons': 0.07600444808798716, 'exampl': 0.044233084254531756, 'respond': 0.07600444808798716, 'head': 0.15200889617597432, 'hurt': 0.15200889617597432, 'whi': 0.07600444808798716, 'say': 0.07600444808798716, '?': 0.07600444808798716, '.': 0.0}, 'During the 1970': {'dure': 0.08065105072186683, '1970': 0.09835869752563044, ',': 0.013622983196285906, 'mani': 0.04222678520567016, 'programm': 0.09835869752563044, 'began': 0.09835869752563044, 'write': 0.08065105072186683, '``': 0.05258509456188669, 'conceptu': 0.09835869752563044, 'ontolog': 0.09835869752563044, \"''\": 0.05258509456188669, 'structur': 0.09835869752563044, 'real-world': 0.06294340391810324, 'inform': 0.0702927413656503, 'computer-understand': 0.09835869752563044, 'data': 0.04222678520567016, '.': 0.0}, 'Examples are MA': {'exampl': 0.03892511414398795, 'margi': 0.0668839143174287, '(': 0.18818740022200092, 'schank': 0.0668839143174287, ',': 0.05558177144084649, '1975': 0.0668839143174287, ')': 0.1765974483776979, 'sam': 0.0668839143174287, 'cullingford': 0.0668839143174287, '1978': 0.1337678286348574, 'pam': 0.0668839143174287, 'wilenski': 0.0668839143174287, 'talespin': 0.0668839143174287, 'meehan': 0.0668839143174287, '1976': 0.0668839143174287, 'qualm': 0.0668839143174287, 'lehnert': 0.1337678286348574, '1977': 0.0668839143174287, 'polit': 0.0668839143174287, 'carbonel': 0.0668839143174287, '1979': 0.0668839143174287, 'plot': 0.0668839143174287, 'unit': 0.0668839143174287, '1981': 0.0668839143174287, '.': 0.0}, 'During this tim': {'dure': 0.11425565518931136, 'thi': 0.08916982221731291, 'time': 0.11425565518931136, ',': 0.02894883929210755, 'mani': 0.059821279041366054, 'chatterbot': 0.13934148816130978, 'written': 0.11425565518931136, 'includ': 0.08916982221731291, 'parri': 0.13934148816130978, 'racter': 0.13934148816130978, 'jabberwacki': 0.13934148816130978, '.': 0.0}, 'Up to the 1980s': {'1980': 0.08916982221731291, ',': 0.009649613097369183, 'natur': 0.04940971765734105, 'languag': 0.038998156273316056, 'process': 0.046512875469073386, 'system': 0.046512875469073386, 'base': 0.08109398779997488, 'complex': 0.11425565518931136, 'set': 0.08916982221731291, 'hand-written': 0.11425565518931136, 'rule': 0.07449555062933948, '.': 0.0}, 'Starting in the': {'start': 0.09140452415144909, 'late': 0.07966510688107033, '1980': 0.07133585777385033, ',': 0.015439380955790694, 'howev': 0.059596440503471584, 'wa': 0.05513332119476404, 'revolut': 0.09140452415144909, 'natur': 0.03952777412587284, 'languag': 0.06239705003730569, 'process': 0.07442060075051743, 'introduct': 0.11147319052904783, 'machin': 0.04785702323309284, 'learn': 0.04204701151849949, 'algorithm': 0.04785702323309284, '.': 0.0}, 'This was due to': {'thi': 0.050954184124178806, 'wa': 0.03938094371054574, 'due': 0.05690364777219309, 'steadi': 0.07962370752074845, 'increas': 0.06528894582246363, 'comput': 0.050954184124178806, 'power': 0.07962370752074845, '(': 0.06400931980340166, 'see': 0.07962370752074845, 'moor': 0.07962370752074845, \"'s\": 0.07962370752074845, 'law': 0.06528894582246363, ')': 0.030033579656071063, 'gradual': 0.07962370752074845, 'lessen': 0.07962370752074845, 'domin': 0.07962370752074845, 'chomskyan': 0.07962370752074845, 'theori': 0.07962370752074845, 'linguist': 0.05690364777219309, 'e.g': 0.07962370752074845, '.': 0.0}, 'transformationa': {'transform': 0.08065105072186683, 'grammar': 0.08065105072186683, ')': 0.037100304281028965, ',': 0.006811491598142953, 'whose': 0.09835869752563044, 'theoret': 0.09835869752563044, 'underpin': 0.09835869752563044, 'discourag': 0.09835869752563044, 'sort': 0.09835869752563044, 'corpu': 0.08065105072186683, 'linguist': 0.0702927413656503, 'underli': 0.09835869752563044, 'machine-learn': 0.06294340391810324, 'approach': 0.0702927413656503, 'languag': 0.02752811031057604, 'process': 0.03283261797816945, '.': 0.0}, '[3] Some of the': {'[': 0.04704982145010914, '3': 0.08800515041766933, ']': 0.04704982145010914, 'earliest-us': 0.07216146643535454, 'machin': 0.03778186044717856, 'learn': 0.033195009093552226, 'algorithm': 0.03778186044717856, ',': 0.012188984965097915, 'decis': 0.056317782453039734, 'tree': 0.07216146643535454, 'produc': 0.04704982145010914, 'system': 0.029376552927835826, 'hard': 0.07216146643535454, 'if-then': 0.07216146643535454, 'rule': 0.09409964290021829, 'similar': 0.07216146643535454, 'exist': 0.07216146643535454, 'hand-written': 0.07216146643535454, '.': 0.0}, 'However, part-o': {'howev': 0.030825745088002544, ',': 0.01996471675317762, 'part-of-speech': 0.04727820214730125, 'tag': 0.04727820214730125, 'introduc': 0.05765854682536957, 'use': 0.026517512791164616, 'hidden': 0.05765854682536957, 'markov': 0.05765854682536957, 'model': 0.05303502558232923, 'natur': 0.020445400409934228, 'languag': 0.0161371681130963, 'process': 0.019246707090651058, 'increasingli': 0.04120608976607086, 'research': 0.02475363270677216, 'ha': 0.026517512791164616, 'focus': 0.04120608976607086, 'statist': 0.02475363270677216, 'make': 0.08241217953214172, 'soft': 0.04727820214730125, 'probabilist': 0.04727820214730125, 'decis': 0.03689785746923293, 'base': 0.03355613288274823, 'attach': 0.04727820214730125, 'real-valu': 0.04727820214730125, 'weight': 0.04727820214730125, 'featur': 0.04120608976607086, 'input': 0.03355613288274823, 'data': 0.02475363270677216, '.': 0.0}, 'The cache langu': {'cach': 0.13934148816130978, 'languag': 0.038998156273316056, 'model': 0.12816797849062897, 'upon': 0.13934148816130978, 'mani': 0.059821279041366054, 'speech': 0.11425565518931136, 'recognit': 0.11425565518931136, 'system': 0.046512875469073386, 'reli': 0.0995813836013379, 'exampl': 0.08109398779997488, 'statist': 0.059821279041366054, '.': 0.0}, 'Such models are': {'model': 0.029577225805529767, 'gener': 0.03438256182892592, 'robust': 0.06431145607445067, 'given': 0.04596063858523288, 'unfamiliar': 0.06431145607445067, 'input': 0.07485598873843836, ',': 0.008907335166802324, 'especi': 0.0527333793181437, 'contain': 0.0527333793181437, 'error': 0.06431145607445067, '(': 0.025849917612912214, 'veri': 0.0527333793181437, 'common': 0.0527333793181437, 'real-world': 0.041155302561836735, 'data': 0.027609821096015105, ')': 0.024257891260672786, 'produc': 0.03438256182892592, 'reliabl': 0.0527333793181437, 'result': 0.03180768530467156, 'integr': 0.06431145607445067, 'larger': 0.04596063858523288, 'system': 0.021467480985726182, 'compris': 0.06431145607445067, 'multipl': 0.06431145607445067, 'subtask': 0.0527333793181437, '.': 0.0}, 'Many of the not': {'mani': 0.03778186044717856, 'notabl': 0.07216146643535454, 'earli': 0.07216146643535454, 'success': 0.1257870108648479, 'occur': 0.08800515041766933, 'field': 0.08800515041766933, 'machin': 0.03778186044717856, 'translat': 0.04352630620639266, ',': 0.012188984965097915, 'due': 0.06289350543242395, 'especi': 0.07216146643535454, 'work': 0.056317782453039734, 'ibm': 0.08800515041766933, 'research': 0.03778186044717856, 'complic': 0.08800515041766933, 'statist': 0.03778186044717856, 'model': 0.04047409847072494, 'develop': 0.056317782453039734, '.': 0.0}, 'These systems w': {'system': 0.046512875469073386, 'abl': 0.06967074408065489, 'take': 0.05712782759465568, 'advantag': 0.04979069180066895, 'exist': 0.05712782759465568, 'multilingu': 0.06967074408065489, 'textual': 0.06967074408065489, 'corpora': 0.04979069180066895, 'produc': 0.03724777531466974, 'parliament': 0.06967074408065489, 'canada': 0.06967074408065489, 'european': 0.06967074408065489, 'union': 0.06967074408065489, 'result': 0.034458325746727525, 'law': 0.05712782759465568, 'call': 0.04979069180066895, 'translat': 0.034458325746727525, 'government': 0.06967074408065489, 'proceed': 0.06967074408065489, 'offici': 0.06967074408065489, 'languag': 0.019499078136658028, 'correspond': 0.06967074408065489, 'govern': 0.06967074408065489, '.': 0.0}, 'However, most o': {'howev': 0.04966370041955965, ',': 0.012866150796492243, 'system': 0.09302575093814677, 'depend': 0.07617043679287423, 'corpora': 0.06638758906755861, 'specif': 0.09289432544087318, 'develop': 0.05944654814487527, 'task': 0.04272265949687632, 'implement': 0.09289432544087318, 'wa': 0.04594443432897003, '(': 0.03733876988531764, 'often': 0.07617043679287423, 'continu': 0.09289432544087318, ')': 0.03503917626541624, 'major': 0.09289432544087318, 'limit': 0.07617043679287423, 'success': 0.06638758906755861, '.': 0.0}, 'As a result, a ': {'result': 0.05907141556581861, ',': 0.008271096940602157, 'great': 0.11943556128112268, 'deal': 0.11943556128112268, 'research': 0.05127538203545662, 'ha': 0.054929133638840985, 'gone': 0.11943556128112268, 'method': 0.09793341873369545, 'effect': 0.11943556128112268, 'learn': 0.045050369484106595, 'limit': 0.09793341873369545, 'amount': 0.07643127618626822, 'data': 0.05127538203545662, '.': 0.0}, 'Recent research': {'recent': 0.16720978579357176, 'research': 0.07178553484963927, 'ha': 0.07690078709437739, 'increasingli': 0.1194976603216055, 'focus': 0.1194976603216055, 'unsupervis': 0.16720978579357176, 'semi-supervis': 0.16720978579357176, 'learn': 0.06307051727774925, 'algorithm': 0.07178553484963927, '.': 0.0}, 'Such algorithms': {'algorithm': 0.059821279041366054, 'learn': 0.05255876439812436, 'data': 0.11964255808273211, 'ha': 0.06408398924531448, 'hand-annot': 0.13934148816130978, 'desir': 0.13934148816130978, 'answer': 0.0995813836013379, 'use': 0.06408398924531448, 'combin': 0.13934148816130978, 'annot': 0.11425565518931136, 'non-annot': 0.11425565518931136, '.': 0.0}, 'Generally, this': {'gener': 0.04966370041955965, ',': 0.012866150796492243, 'thi': 0.05944654814487527, 'task': 0.04272265949687632, 'much': 0.06638758906755861, 'difficult': 0.09289432544087318, 'supervis': 0.09289432544087318, 'learn': 0.03503917626541624, 'typic': 0.07617043679287423, 'produc': 0.04966370041955965, 'less': 0.09289432544087318, 'accur': 0.09289432544087318, 'result': 0.04594443432897003, 'given': 0.06638758906755861, 'amount': 0.05944654814487527, 'input': 0.05406265853331659, 'data': 0.03988085269424403, '.': 0.0}, 'However, there ': {'howev': 0.029798220251735792, ',': 0.015439380955790694, 'enorm': 0.05573659526452392, 'amount': 0.03566792888692517, 'non-annot': 0.045702262075724545, 'data': 0.02392851161654642, 'avail': 0.05573659526452392, '(': 0.022403261931190584, 'includ': 0.03566792888692517, 'among': 0.05573659526452392, 'thing': 0.05573659526452392, 'entir': 0.05573659526452392, 'content': 0.05573659526452392, 'world': 0.045702262075724545, 'wide': 0.05573659526452392, 'web': 0.05573659526452392, ')': 0.021023505759249744, 'often': 0.045702262075724545, 'make': 0.039832553440535164, 'inferior': 0.05573659526452392, 'result': 0.02756666059738202, 'algorithm': 0.02392851161654642, 'use': 0.025633595698125795, 'ha': 0.025633595698125795, 'low': 0.05573659526452392, 'enough': 0.05573659526452392, 'time': 0.045702262075724545, 'complex': 0.045702262075724545, 'practic': 0.05573659526452392, '.': 0.0}, 'In the 2010s, r': {'2010': 0.04777422451244907, ',': 0.016542193881204315, 'represent': 0.04777422451244907, 'learn': 0.036040295587285276, 'deep': 0.03414218866331585, 'neural': 0.03414218866331585, 'network-styl': 0.04777422451244907, 'machin': 0.020510152814182645, 'method': 0.039173367493478176, 'becam': 0.04777422451244907, 'widespread': 0.04777422451244907, 'natur': 0.03388094925074815, 'languag': 0.04011238930969652, 'process': 0.01594727158939659, 'due': 0.03414218866331585, 'part': 0.04777422451244907, 'flurri': 0.04777422451244907, 'result': 0.04725713245265489, 'show': 0.04777422451244907, 'techniqu': 0.039173367493478176, '[': 0.1277066582217248, '4': 0.04777422451244907, ']': 0.1277066582217248, '5': 0.04777422451244907, 'achiev': 0.04777422451244907, 'state-of-the-art': 0.04777422451244907, 'mani': 0.04102030562836529, 'task': 0.021971653455536394, 'exampl': 0.02780365295999139, 'model': 0.021971653455536394, '6': 0.04777422451244907, 'pars': 0.039173367493478176, '7': 0.04777422451244907, '8': 0.04777422451244907, '.': 0.0}, 'Popular techniq': {'popular': 0.05573659526452392, 'techniqu': 0.045702262075724545, 'includ': 0.03566792888692517, 'use': 0.025633595698125795, 'word': 0.07966510688107033, 'embed': 0.05573659526452392, 'captur': 0.05573659526452392, 'semant': 0.05573659526452392, 'properti': 0.05573659526452392, ',': 0.011579535716843021, 'increas': 0.045702262075724545, 'end-to-end': 0.05573659526452392, 'learn': 0.021023505759249744, 'higher-level': 0.05573659526452392, 'task': 0.05126719139625159, '(': 0.04480652386238117, 'e.g.': 0.11147319052904783, 'question': 0.05573659526452392, 'answer': 0.039832553440535164, ')': 0.04204701151849949, 'instead': 0.045702262075724545, 'reli': 0.039832553440535164, 'pipelin': 0.05573659526452392, 'separ': 0.05573659526452392, 'intermedi': 0.045702262075724545, 'part-of-speech': 0.045702262075724545, 'tag': 0.045702262075724545, 'depend': 0.045702262075724545, 'pars': 0.045702262075724545, '.': 0.0}, 'In some areas, ': {'area': 0.0668839143174287, ',': 0.009263628573474417, 'thi': 0.0428015146643102, 'shift': 0.0668839143174287, 'ha': 0.030760314837750956, 'entail': 0.0668839143174287, 'substanti': 0.0668839143174287, 'chang': 0.0668839143174287, 'nlp': 0.047799064128642196, 'system': 0.02232618022515523, 'design': 0.05484271449086945, 'deep': 0.047799064128642196, 'neural': 0.047799064128642196, 'network-bas': 0.0668839143174287, 'approach': 0.047799064128642196, 'may': 0.0668839143174287, 'view': 0.0668839143174287, 'new': 0.0668839143174287, 'paradigm': 0.05484271449086945, 'distinct': 0.0668839143174287, 'statist': 0.028714213939855706, 'natur': 0.023716664475523706, 'languag': 0.01871911501119171, 'process': 0.02232618022515523, '.': 0.0}, 'For instance, t': {'instanc': 0.053938640578571534, ',': 0.007470668204414851, 'term': 0.053938640578571534, 'neural': 0.03854763236180822, 'machin': 0.06946987243513478, 'translat': 0.08003224044401232, '(': 0.043361152124885, 'nmt': 0.053938640578571534, ')': 0.04069065630822531, 'emphas': 0.053938640578571534, 'fact': 0.053938640578571534, 'deep': 0.03854763236180822, 'learning-bas': 0.053938640578571534, 'approach': 0.03854763236180822, 'directli': 0.053938640578571534, 'learn': 0.020345328154112656, 'sequence-to-sequ': 0.053938640578571534, 'transform': 0.04422799555715278, 'obviat': 0.053938640578571534, 'need': 0.04422799555715278, 'intermedi': 0.04422799555715278, 'step': 0.053938640578571534, 'word': 0.03854763236180822, 'align': 0.053938640578571534, 'languag': 0.015096060492896538, 'model': 0.024806705514315287, 'wa': 0.02667741348133744, 'use': 0.024806705514315287, 'statist': 0.023156624145044925, 'smt': 0.053938640578571534, '.': 0.0}, 'In the early da': {'earli': 0.06528894582246363, 'day': 0.07962370752074845, ',': 0.005514064627068105, 'mani': 0.034183588023637745, 'language-process': 0.07962370752074845, 'system': 0.02657878598232765, 'design': 0.06528894582246363, 'hand-cod': 0.07962370752074845, 'set': 0.050954184124178806, 'rule': 0.08513777214781655, ':': 0.06528894582246363, '[': 0.08513777214781655, '9': 0.07962370752074845, ']': 0.08513777214781655, '10': 0.07962370752074845, 'write': 0.06528894582246363, 'grammar': 0.06528894582246363, 'devis': 0.07962370752074845, 'heurist': 0.07962370752074845, 'stem': 0.07962370752074845, '.': 0.0}, 'Since the so-ca': {'sinc': 0.0668839143174287, 'so-cal': 0.0668839143174287, '``': 0.03575786430208295, 'statist': 0.028714213939855706, 'revolut': 0.05484271449086945, \"''\": 0.03575786430208295, '[': 0.0715157286041659, '11': 0.0668839143174287, ']': 0.0715157286041659, '12': 0.0668839143174287, 'late': 0.047799064128642196, '1980': 0.0428015146643102, 'mid-1990': 0.0668839143174287, ',': 0.004631814286737208, 'much': 0.047799064128642196, 'natur': 0.023716664475523706, 'languag': 0.01871911501119171, 'process': 0.02232618022515523, 'research': 0.028714213939855706, 'ha': 0.030760314837750956, 'reli': 0.047799064128642196, 'heavili': 0.0668839143174287, 'machin': 0.028714213939855706, 'learn': 0.025228206911099695, '.': 0.0}, 'The machine-lea': {'machine-learn': 0.03689785746923293, 'paradigm': 0.04727820214730125, 'call': 0.04120608976607086, 'instead': 0.04727820214730125, 'use': 0.026517512791164616, 'statist': 0.02475363270677216, 'infer': 0.05765854682536957, 'automat': 0.04727820214730125, 'learn': 0.021748454233706634, 'rule': 0.030825745088002544, 'analysi': 0.05765854682536957, 'larg': 0.04120608976607086, 'corpora': 0.04120608976607086, '(': 0.023175788204679915, 'plural': 0.05765854682536957, 'form': 0.05765854682536957, 'corpu': 0.04727820214730125, ',': 0.007985886701271048, 'set': 0.03689785746923293, 'document': 0.05765854682536957, 'possibl': 0.04727820214730125, 'human': 0.04120608976607086, 'comput': 0.03689785746923293, 'annot': 0.04727820214730125, ')': 0.021748454233706634, 'typic': 0.04727820214730125, 'real-world': 0.03689785746923293, 'exampl': 0.03355613288274823, '.': 0.0}, 'Many different ': {'mani': 0.07976170538848806, 'differ': 0.15234087358574847, 'class': 0.18578865088174637, 'machine-learn': 0.11889309628975055, 'algorithm': 0.07976170538848806, 'appli': 0.18578865088174637, 'natural-language-process': 0.18578865088174637, 'task': 0.08544531899375264, '.': 0.0}, 'These algorithm': {'algorithm': 0.0652595771360357, 'take': 0.12464253293379421, 'input': 0.17693233701812702, 'larg': 0.108634236656005, 'set': 0.0972761696916141, '``': 0.0812678734138249, 'featur': 0.108634236656005, \"''\": 0.0812678734138249, 'gener': 0.0812678734138249, 'data': 0.0652595771360357, '.': 0.0}, 'Some of the ear': {'earliest-us': 0.09793341873369545, 'algorithm': 0.05127538203545662, ',': 0.016542193881204315, 'decis': 0.07643127618626822, 'tree': 0.09793341873369545, 'produc': 0.0638533291108624, 'system': 0.07973635794698296, 'hard': 0.09793341873369545, 'if-then': 0.09793341873369545, 'rule': 0.1277066582217248, 'similar': 0.09793341873369545, 'handwritten': 0.11943556128112268, 'common': 0.09793341873369545, '.': 0.0}, 'Increasingly, h': {'increasingli': 0.06289350543242395, ',': 0.02437796993019583, 'howev': 0.04704982145010914, 'research': 0.03778186044717856, 'ha': 0.04047409847072494, 'focus': 0.06289350543242395, 'statist': 0.03778186044717856, 'model': 0.04047409847072494, 'make': 0.06289350543242395, 'soft': 0.07216146643535454, 'probabilist': 0.07216146643535454, 'decis': 0.056317782453039734, 'base': 0.05121725545261572, 'attach': 0.07216146643535454, 'real-valu': 0.07216146643535454, 'weight': 0.07216146643535454, 'input': 0.05121725545261572, 'featur': 0.06289350543242395, '.': 0.0}, 'Such models hav': {'model': 0.07323884485178798, 'advantag': 0.05690364777219309, 'express': 0.07962370752074845, 'rel': 0.07962370752074845, 'certainti': 0.07962370752074845, 'mani': 0.034183588023637745, 'differ': 0.06528894582246363, 'possibl': 0.06528894582246363, 'answer': 0.05690364777219309, 'rather': 0.07962370752074845, 'onli': 0.07962370752074845, 'one': 0.07962370752074845, ',': 0.005514064627068105, 'produc': 0.042568886073908276, 'reliabl': 0.06528894582246363, 'result': 0.03938094371054574, 'includ': 0.050954184124178806, 'compon': 0.07962370752074845, 'larger': 0.05690364777219309, 'system': 0.02657878598232765, '.': 0.0}, 'Systems based o': {'system': 0.031008583646048925, 'base': 0.05406265853331659, 'machine-learn': 0.05944654814487527, 'algorithm': 0.03988085269424403, 'mani': 0.03988085269424403, 'advantag': 0.06638758906755861, 'hand-produc': 0.09289432544087318, 'rule': 0.04966370041955965, ':': 0.07617043679287423, 'follow': 0.09289432544087318, 'list': 0.09289432544087318, 'commonli': 0.07617043679287423, 'research': 0.03988085269424403, 'task': 0.04272265949687632, 'natur': 0.032939811771560704, 'languag': 0.02599877084887737, 'process': 0.031008583646048925, '.': 0.0}, 'Some of these t': {'task': 0.11830890322211907, 'direct': 0.12862291214890134, 'real-world': 0.08231060512367347, 'applic': 0.12862291214890134, ',': 0.008907335166802324, 'commonli': 0.1054667586362874, 'serv': 0.12862291214890134, 'subtask': 0.1054667586362874, 'use': 0.05915445161105953, 'aid': 0.12862291214890134, 'solv': 0.1054667586362874, 'larger': 0.09192127717046576, '.': 0.0}, 'Though natural ': {'though': 0.12862291214890134, 'natur': 0.0456089701452379, 'languag': 0.03599829809844559, 'process': 0.042934961971452364, 'task': 0.05915445161105953, 'close': 0.12862291214890134, 'intertwin': 0.12862291214890134, ',': 0.008907335166802324, 'frequent': 0.1054667586362874, 'subdivid': 0.12862291214890134, 'categori': 0.12862291214890134, 'conveni': 0.12862291214890134, '.': 0.0}, 'A coarse divisi': {'coars': 0.4180244644839294, 'divis': 0.4180244644839294, 'given': 0.29874415080401373, '.': 0.0}, 'The first publi': {'first': 0.08065105072186683, 'publish': 0.16130210144373366, 'work': 0.06294340391810324, 'artifici': 0.08065105072186683, 'intellig': 0.0702927413656503, 'wa': 0.04864704811302709, '2018': 0.09835869752563044, ',': 0.02043447479442886, '1': 0.09835869752563044, 'road': 0.09835869752563044, 'market': 0.09835869752563044, 'novel': 0.09835869752563044, 'contain': 0.08065105072186683, 'sixti': 0.08065105072186683, 'million': 0.09835869752563044, 'word': 0.0702927413656503, '.': 0.0}}\n",
      "{'Natural languag': 0.05437194740348606, 'Challenges in n': 0.0951030725970823, 'The history of ': 0.058817313067862864, 'In 1950, Alan T': 0.06616945947801593, 'The Georgetown ': 0.11558950284921314, 'The authors cla': 0.09709172336889799, '[2]  However, r': 0.05237778541281753, 'Little further ': 0.0826088240707312, 'Some notably su': 0.04955665544439431, 'Using almost no': 0.08798191790774564, 'When the \"patie': 0.07416890572402637, 'During the 1970': 0.06978211012578235, 'Examples are MA': 0.07722951396671857, 'During this tim': 0.09727022341510605, 'Up to the 1980s': 0.06279364426739466, 'Starting in the': 0.059303921935951597, 'This was due to': 0.06455134196837725, 'transformationa': 0.06705333812568848, '[3] Some of the': 0.05447508279208312, 'However, part-o': 0.03796256256248752, 'The cache langu': 0.08509926886819213, 'Such models are': 0.04417092545063418, 'Many of the not': 0.058796732295316935, 'These systems w': 0.05537420281532038, 'However, most o': 0.06160616558627149, 'As a result, a ': 0.07285093846274508, 'Recent research': 0.10241670520953314, 'Such algorithms': 0.09219231153972846, 'Generally, this': 0.057181074774203866, 'However, there ': 0.04079580362973296, 'In the 2010s, r': 0.04328660008529044, 'Popular techniq': 0.04752709154601667, 'In some areas, ': 0.046733945624920475, 'For instance, t': 0.042259729996136905, 'In the early da': 0.0636227775466175, 'Since the so-ca': 0.04273668123300807, 'The machine-lea': 0.038688536032455985, 'Many different ': 0.11928540581016299, 'These algorithm': 0.09004020795173559, 'Some of the ear': 0.08018437650469963, 'Increasingly, h': 0.052582808235834284, 'Such models hav': 0.05696966813441255, 'Systems based o': 0.05243918408699014, 'Some of these t': 0.0916534228306606, 'Though natural ': 0.08229294219405332, 'A coarse divisi': 0.2836982699429681, 'The first publi': 0.07921581759069665}\n",
      "0.07250979660557874\n",
      " Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky. Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. A coarse division is given below.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    result = run_summarization(text_str)                                      #Print the final result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim_sum_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY USING GenSim Text Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was due to both the steady increase in computational power (see Moore\\'s law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\\nHowever, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\\nSuch models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\\nGenerally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\\nSince the so-called \"statistical revolution\"[11][12] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.\\nThe machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\\nMany different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\nIncreasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.\\nSystems based on machine-learning algorithms have many advantages over hand-produced rules:'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize                       # summarize using GENSIM\n",
    "summarize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'real',\n",
       " 'languages',\n",
       " 'systems',\n",
       " 'results',\n",
       " 'result',\n",
       " 'worlds',\n",
       " 'world',\n",
       " 'tasks',\n",
       " 'task',\n",
       " 'research',\n",
       " 'researched',\n",
       " 'statistical',\n",
       " 'base',\n",
       " 'based',\n",
       " 'rules',\n",
       " 'computers',\n",
       " 'computing',\n",
       " 'computational',\n",
       " 'natural language processing',\n",
       " 'translation',\n",
       " 'machine',\n",
       " 'process',\n",
       " 'word',\n",
       " 'words',\n",
       " 'answers',\n",
       " 'answering',\n",
       " 'hand',\n",
       " 'large',\n",
       " 'intelligence',\n",
       " 'human',\n",
       " 'produced',\n",
       " 'produce',\n",
       " 'produces',\n",
       " 'producing',\n",
       " 'input',\n",
       " 'generation',\n",
       " 'generally',\n",
       " 'generic',\n",
       " 'generated',\n",
       " 'including',\n",
       " 'include',\n",
       " 'included',\n",
       " 'paradigm',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'successful',\n",
       " 'successively',\n",
       " 'success',\n",
       " 'written',\n",
       " 'lehnert',\n",
       " 'possibly',\n",
       " 'possible',\n",
       " 'turing published',\n",
       " 'models',\n",
       " 'modeling',\n",
       " 'model',\n",
       " 'semantic',\n",
       " 'underpinnings',\n",
       " 'substantial',\n",
       " 'multiple',\n",
       " 'contains',\n",
       " 'reduced',\n",
       " 'neural',\n",
       " 'early successes',\n",
       " 'information',\n",
       " 'transformational',\n",
       " 'transformations',\n",
       " 'intermediate',\n",
       " 'increasingly',\n",
       " 'corpora',\n",
       " 'example',\n",
       " 'examples',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'titled',\n",
       " 'speech',\n",
       " 'annotated',\n",
       " 'annotations']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "keywords(mytext, split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary after TF-IDF and Weighted Frequency**\n",
    "\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky. Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. A coarse division is given below.\n",
    "\n",
    "## **Summary after GenSim Text Rank**\n",
    "\n",
    "'This was due to both the steady increase in computational power (see Moore\\'s law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\\nHowever, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\\nSuch models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\\nGenerally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\\nSince the so-called \"statistical revolution\"[11][12] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.\\nThe machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\\nMany different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\nIncreasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.\\nSystems based on machine-learning algorithms have many advantages over hand-produced rules:'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above summaries for both TF-IDF and GENSIM TEXT RANK it is clear that the summary for TF-IDF is much precise and short as compared to the one by GenSim Text Rank. TF-IDF summary does not have any punctuations and digits, its pure text with the words that have the maximum frequency in the original text. GenSim Text Rank gives a summary longer than TF-IDF. Also it includes \\n which indicates new lines, includes digits and punctuations like brackets which is not in weighted frequency and TF-IDF summary.The weighted frequenxy of the words is taken to build summary while in GenSim text Rank it does not take the weighted frequency of words into consideration for summarizing. In TF-IDF we perform tokenization and remove stop words, punctuations and use word frequencies to build the summaries while for Gensim-Text rank it just uses the frequency of the words and sentences to build the summary. Hence TF-IDF is better for text summarization**\n",
    "\n",
    "## **IMPROVEMENT**\n",
    "\n",
    "Text summarization can be improved using **N-Gram** models with NLTK for text summarization as it gives precise summarization compared to the word frequency, TD-IDF and GenSim Text Rank summarization. For word frequency summarization digits and punctuations are included and the summary like brackets and it uses just the most frequenctly used words and the summary is big . While by using  N-Grams the summary is small and does not include punctuations and uses probability of the words and prediction in the summary. It does not include un-necessary digits and brackets in the summary. The traditional summarization methods classify the sentences based on the word frequency, positon in the content and cue expressions. N-Gram models are better than these traditional methods. We do the summarization in N-Gram method by summariizng it and finding word frequency and then perform wprd prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
